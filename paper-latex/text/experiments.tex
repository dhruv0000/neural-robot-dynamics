\section{Experiments}
\label{sec:experiments}

We train \textit{NeRD} models on six distinct robotic systems (Fig. \ref{fig:tasks}) and conduct extensive experiments to validate the capabilities of the trained \textit{NeRD} models \footnote{View the supplementary video to observe qualitative performance throughout the following examples.}. We investigate the following questions: Can \textit{NeRD} reliably and accurately simulate long-horizon robotic trajectories (\S\ref{sec:exp-passive})? Does \textit{NeRD}'s hybrid prediction framework enable it to generalize across different contact configurations (\S\ref{sec:exp-contact})?  Can a single \textit{NeRD} model generalize to diverse tasks, customized robotic controllers, and spatial regions that are unseen during training? Can we train robotic policies for diverse tasks entirely in a \textit{NeRD} simulator and successfully deploy the learned policies in the ground-truth simulator and even in the real world (\S\ref{sec:exp-policy-learning}, \S\ref{sec:exp-sim-to-real})? Finally, can we effectively fine-tune the pretrained \textit{NeRD} models from real-world data (\S\ref{sec:exp-cube-tossing})? We also provide a comprehensive ablation study in Appendix~\ref{sup:ablation}, highlighting critical design decisions essential for successfully training \textit{NeRD} models.

\subsection{Long-Horizon Stability and Accuracy: Cartpole and Ant with Passive Motion}
\label{sec:exp-passive}
We first evaluate \textit{NeRD}'s long-horizon performance using open-loop passive motions of \textit{Cartpole} and \textit{Ant}. While \textit{Cartpole} is a contact-free system that serves as a tractable problem for analysis, \textit{Ant} assesses \textit{NeRD}'s performance when a floating base, high DoFs ($14$), and contact are all involved. The robots start from randomized initial states and apply zero joint torques, $\bm{\tau}_t=0$. We compute the temporally averaged errors between the trajectories generated by \textit{NeRD} and the ground-truth simulators. We report the errors averaged from $2048$ trajectories for each test in Fig.~\ref{fig:passive_motion_eval} (left).  

For \textit{Cartpole}, we evaluate trajectories of $100$, $500$, and $1000$ steps. We measure the errors of the prismatic base joint (reported as \textit{Base Position Err.}) and the non-base revolute joint. As a reference for the prismatic joint error, the pole length is $1$ \textit{m}. 
To visualize the simulation accuracy, Fig.~\ref{fig:passive_motion_eval} (right) compares the state trajectories generated by \textit{NeRD} and the ground-truth simulator from the same initial state. 
The results demonstrate high long-horizon accuracy of \textit{NeRD} on \textit{Cartpole}, with accumulated error of $0.075$ \textit{rad} (smaller than $5^\circ$) for the revolute joint and $0.033$ \textit{m} for the prismatic joint, even after $1000$ steps (\ie equivalent to $16.67$ seconds of passive motion). 
% Even in the $1000$-step stress test (\ie equivalent to $16.67$ seconds of passive motion), \textit{NeRD} shows an accumulated error of $0.075$ \textit{rad} (smaller than $5^\circ$) on the revolute joint and $0.033 m$ on the prismatic joint. 
For \textit{Ant}, we evaluate on $500$-step trajectories, as the motion typically converges to a static state within this duration. We measure the base's position and orientation errors, and the mean error of non-base revolute joints. \textit{NeRD} achieves an average base angular error of  $0.095$ \textit{rad} and positional error of $0.057$ \textit{m} after $500$ steps of simulation ($1.2\,m$ full body width). The minimal prediction errors indicate \textit{NeRD}'s capability to accurately predict the motion of robots with a floating base for extended horizons.


\begin{figure}[t!]
    \begin{minipage}[c]{0.59\textwidth}
        \vspace{0pt}
        \centering
        % \captionof{table}{Evaluation of \textit{NeRD} on long-horizon passive motions.}
        \resizebox{\textwidth}{!}{
            % \begin{tabular}{lcccc}
            % \toprule
            % \textbf{Robot} & \multicolumn{3}{c}{\textbf{Cartpole}} & \textbf{Ant} \\ 
            % \midrule
            % \textbf{Trajectory Horizon} & 100 & 500 & 1000 & 500 \\ 
            % \midrule
            % \midrule
            % % Base Position Err. (\textit{m}) &  $1.6 \times 10^{-4}$  &  $0.004$  & $0.033$ & $0.057$ \\
            % Base Position Err. (\textit{m}) &  $0.0002$  &  $0.004$  & $0.033$ & $0.057$ \\
            % \midrule
            % Base Orientation Err. (\textit{rad}) &  -  &  -  & -  & $0.095$\\
            % \midrule
            % % Non-Base Joint Err. (\textit{rad}) & $4.0\times10^{-4}$ & $0.013$ & $0.075$ & $0.077$ \\
            % Non-Base Joint Err. (\textit{rad}) & $0.0004$ & $0.013$ & $0.075$ & $0.077$ \\
            % \bottomrule
            % \end{tabular}

            \begin{tabular}{lcccc}
                \toprule
                \textbf{Robot} & \multicolumn{3}{c}{\textbf{Cartpole}} & \textbf{Ant} \\ 
                \cmidrule(lr){2-4}\cmidrule(lr){5-5}
                \textbf{Trajectory Horizon} & 100 & 500 & 1000 & 500 \\ 
                \midrule
                Base Position Err. (\textit{m}) & $0.0002$ & $0.004$ & $0.033$ & $0.057$ \\
                Base Orientation Err. (\textit{rad}) & - & - & - & $0.095$\\
                Non-Base Joint Err. (\textit{rad}) & $0.0004$ & $0.013$ & $0.075$ & $0.077$ \\
                \bottomrule
            \end{tabular}
        }
        % \label{tab:passive_motion_eval}
    \end{minipage} %\hfill
    \begin{minipage}[c]{0.45\textwidth}
        \vspace{0pt}
        \centering
        % \includegraphics[width=0.8\textwidth]{figures/cartpole_plot_v7.pdf}
        \includegraphics[width=0.8\textwidth]{figures/cartpole_plot_v10.pdf}
        % \captionof{figure}{State trajectories of Cartpole by \textit{NeRD} and ground-truth simulator.}
        % \label{fig:cartpole_states}
    \end{minipage}
\captionof{figure}{\textbf{Evaluation of \textit{NeRD} on long-horizon passive motions.} Left: Full report of the measured errors. Right: $1000$-step cartpole state trajectories simulated by \textit{NeRD} and ground-truth simulator. }
% add side notes about prismatic joint shift
\label{fig:passive_motion_eval}
% \vspace{-1em}
\end{figure}

\subsection{Contact Generalizability: Double Pendulum with Varying Contact Environments}
\label{sec:exp-contact}

We validate \textit{NeRD}'s generalizability across varying contact configurations using a \textit{Double Pendulum} example, in which a randomized planar ground (random normal direction and position) is placed beneath the double pendulum. Different combinations of ground configurations and initial pendulum states yield distinct modes of motion: \textit{contact-free chaotic motion} \cite{shinbrot1992chaos} when the ground is distant; \textit{sliding contact motion}, occurring when the pendulum lightly touches and slides along the ground surface; and \textit{collision-induced stopping motion} when the ground is positioned closely enough that the pendulum rapidly comes to rest after contact. Such varied contact configurations pose challenges for prior methods, as typical state representation without encoding the environment provides insufficient clues to determine contact timing and mode. To test \textit{NeRD}, we evaluate seven different ground setups -- one contact-free and six involving potential pendulum-ground contact. For each ground configuration, $2048$ passive-motion trajectories of $100$ steps are simulated with random initial states of the pendulum and zero joint torques.
% For each ground configuration, $2048$ motion trajectories are evaluated, and the mean temporally averaged joint error is computed. 
Due to space constraints,  visualizations of the seven ground configurations and detailed error metrics for \textit{NeRD} are provided in the Appendix~\ref{sup:pendulum_results}.
% Three representative configurations and their associated errors are presented in Fig.~\ref{fig:pendulum_with_differnt_grounds}.
% We visualize three ground configurations in Fig. \ref{fig:pendulum_with_differnt_grounds}, and report the measured errors of these three configurations in Table \ref{tab:passive_motion_eval}. 
% We provide a full report of the error metrics on all seven contact configurations in the Appendix. 
Among all seven ground configurations, the maximum mean joint error after $100$-step simulation is $0.056$ \textit{rad} ($3.2^\circ$), with joint errors typically below $1^\circ$ for most cases. The results demonstrate that a single \textit{NeRD} model effectively generalizes across diverse contact scenarios.
%and maintains high prediction accuracy over extended simulation horizons.

% Appendix
% \begin{figure}[h!]
%     \centering
%     \includegraphics[width=\linewidth]{figures/pendulum_with_contacts_v2.pdf}
    
%     \caption{\textbf{Three different ground setups to evaluate \textit{NeRD}'s generalizability across contact configurations}: (a) \textbf{no contact}; (b) \textbf{ground \#1} where the ground has a normal vector pointing upward  and is positioned such that the pendulum can make contact; 
%     % and passes through the origin;
%     (c) \textbf{ground \#2} where the ground has a non-axis-aligned normal. \todo{add errors}}
%     \label{fig:pendulum_with_differnt_grounds}
% \end{figure}

\subsection{Task, Controller, and Spatial Generalizability: Robotic Policy Learning via RL}
\label{sec:exp-policy-learning}
% Our trained \textit{NeRD} models can reliably predict dynamics under novel robot state distributions induced by unseen tasks, be flexible with customized robotic low-level controllers, and generalize to spatial regions of the robots that are uncovered during \textit{NeRD} training. 
To evaluate the task, controller, and spatial generalizability of \textit{NeRD}, we conduct extensive RL policy-learning experiments across diverse tasks for four robotic systems: a swing-up task for \textit{Cartpole}, an end-effector reach task for \textit{Franka}, three different tasks (running, spinning, and spin tracking) for \textit{Ant}, and forward and sideways velocity-tracking for \textit{ANYmal}~\cite{hutter2016anymal}. While detailed task descriptions are provided in the Appendix~\ref{sup:tasks}, we highlight key aspects here. First, each task explores specialized robot state distributions that is never covered in the \textit{NeRD} training dataset for that robot, which \textit{only} includes randomly-generated motions. Conducting policy learning in those tasks requires the trained \textit{NeRD} models to make accurate predictions under unseen state distributions. Second, to verify trained \textit{NeRD} models' generalizability to low-level controllers, we use \textit{joint-torque control} for \textit{Cartpole} and \textit{Ant}, and \textit{joint-position control} for \textit{Franka} and \textit{ANYmal}. Third, in the \textit{Ant} running task and \textit{ANYmal} velocity-tracking tasks, the robots reach a spatial region that is exceptionally far from the range covered by the training datasets, thus examining \textit{NeRD}'s spatial generalizability. Fourth, the horizons of the tasks vary from several hundred steps to $1000$ (\textit{ANYmal} tasks), assessing the stability and accuracy of the \textit{NeRD} models over extremely long horizons. 

For each task, we use PPO \cite{schulman2017proximal} to train three policies with different random seeds \textit{entirely} within the \textit{NeRD} simulators. We then evaluate each learned policy over $2048$ trajectories in \textit{both} the \textit{NeRD} and the ground-truth simulators, and report the average rewards and the standard deviations in Table~\ref{tab:policy_eval}. Despite training purely from random trajectories, the results show that the trained \textit{NeRD} models can support high-performing policy learning for diverse tasks (see supplementary video for policy behaviors). Furthermore, the \textit{NeRD}-trained policies have remarkably similar rewards when deployed in the \textit{NeRD} simulator and in the ground-truth simulator (without any fine-tuning or adaptation phase), further confirming the long-horizon predictive accuracy of \textit{NeRD} models.

% \begin{table*}[t!]
% \caption{Quantitative evaluation of policies trained exclusively in \textit{NeRD} simulators, when deployed in both \textit{NeRD} simulators \textit{and} the ground-truth simulator.}
% \centering
% \label{tab:policy_eval}
% % \begin{tiny}
% {\fontsize{6.3pt}{6.8pt}\selectfont
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Robot}    & \textbf{Cartpole}         & \textbf{Franka}       & \multicolumn{3}{c}{\textbf{Ant}}                                    & \multicolumn{2}{c}{\textbf{ANYmal}} \\ 
% \midrule
% \textbf{Task}     & Swing Up                  & Reach                 & Running               & Spinning            & Spin Tracking         & Forward Walk       & Sideways Walk \\
% \midrule
% \midrule
% GT Reward         & 1212.5 $\pm$ 210.4        & 85.9 $\pm$ 11.8      & 2541.5 $\pm$ 309.1    & 2624.7 $\pm$ 641.0  & 1630.2 $\pm$ 203.1    & 1323.4 $\pm$ 60.5  & 1360.2 $\pm$ 81.2  \\
% \midrule
% NeRD Reward       & 1212.6 $\pm$ 210.2        & 91.5 $\pm$ 10.0      & 2649.5 $\pm$ 227.4    & 3076.2 $\pm$ 433.5  & 1670.5 $\pm$ 192.6    & 1323.1 $\pm$ 62.4  & 1359.2 $\pm$ 70.4  \\
% \midrule
% Reward Err. (\%) & +0.01\%                   & +6.44\%               & +4.25\%               & +17.21\%            & +2.47\%               & -0.02\%            & -0.07\%            \\
% \bottomrule
% \end{tabular}
% }
% % \end{tiny}
% \end{table*}

\begin{table*}[t!]
\caption{Quantitative evaluation of policies trained exclusively in \textit{NeRD} simulators, when deployed in both \textit{NeRD} simulators \textit{and} the ground-truth simulator.}
\centering
\label{tab:policy_eval}
% \begin{tiny}
{
\fontsize{6.8pt}{7.5pt}\selectfont
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lccccccc}
\toprule
\textbf{Robot}    & \textbf{Cartpole}         & \textbf{Franka}       & \multicolumn{3}{c}{\textbf{Ant}}                                    & \multicolumn{2}{c}{\textbf{ANYmal}} \\ 
\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-6}\cmidrule(lr){7-8}
\textbf{Task}     & Swing Up                  & Reach                 & Running               & Spinning            & Spin Tracking         & Forward Walk       & Sideways Walk \\
% \cmidrule(lr){1-8}
\midrule
% \midrule
% GT Reward         & 1212.5 $\pm$ 210.4        & 85.9 $\pm$ 11.8      & 2541.5 $\pm$ 309.1    & 2624.7 $\pm$ 641.0  & 1630.2 $\pm$ 203.1    & 1323.4 $\pm$ 60.5  & 1360.2 $\pm$ 81.2  \\
GT Reward         & 1212.5 $\pm$ 210.4        & 89.3 $\pm$ 10.5      & 2541.5 $\pm$ 309.1    & 2624.7 $\pm$ 641.0  & 1630.2 $\pm$ 203.1    & 1323.4 $\pm$ 60.5  & 1360.2 $\pm$ 81.2  \\
% \midrule
\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-6}\cmidrule(lr){7-8}
% NeRD Reward       & 1212.6 $\pm$ 210.2        & 91.5 $\pm$ 10.0      & 2649.5 $\pm$ 227.4    & 3076.2 $\pm$ 433.5  & 1670.5 $\pm$ 192.6    & 1323.1 $\pm$ 62.4  & 1359.2 $\pm$ 70.4  \\
NeRD Reward       & 1212.6 $\pm$ 210.2        & 91.1 $\pm$ 9.9      & 2649.5 $\pm$ 227.4    & 3076.2 $\pm$ 433.5  & 1670.5 $\pm$ 192.6    & 1323.1 $\pm$ 62.4  & 1359.2 $\pm$ 70.4  \\
% \midrule
\cmidrule(lr){2-2}\cmidrule(lr){3-3}\cmidrule(lr){4-6}\cmidrule(lr){7-8}
% Reward Err. (\%) & +0.01\%                   & +6.44\%               & +4.25\%               & +17.21\%            & +2.47\%               & -0.02\%            & -0.07\%            \\
Reward Err. (\%) & +0.01\%                   & +2.02\%               & +4.25\%               & +17.21\%            & +2.47\%               & -0.02\%            & -0.07\%            \\
\bottomrule
\end{tabular}
}
% \end{tiny}
% \vspace{-1em}
\end{table*}

\subsection{Sim-to-Real Transfer of Franka Reach Policy}
\label{sec:exp-sim-to-real}
We further evaluate the accuracy of \textit{NeRD} models via zero-shot sim-to-real transfer of a \textit{Franka} reach policy trained exclusively in the \textit{NeRD} simulator in \S\ref{sec:exp-policy-learning} (Fig.~\ref{fig:franka_reach}). The goal of this task is to move the robot’s end-effector to a randomly-specified target position. The robot is controlled via a \textit{joint-position controller}. See the Appendix for detailed task and reward settings. We command the robot to move to 50 different target positions sampled within the robot’s workspace, and we evaluate the policy's performance by measuring the distance to the targets at steady-state. As a baseline, we repeat the same experiment using a policy trained in the ground-truth simulator. Deployment results show that policies trained with both \textit{NeRD} and the ground-truth (\textbf{GT}) simulator achieve low steady-state error, with mean and standard deviation: \textbf{NeRD:} $1.927 \pm 0.699$ \textit{mm}, \textbf{GT:} $4.647 \pm 2.667$ \textit{mm}. We show the distance-to-goal plots of ten executions of \textit{NeRD}-trained policies in Fig.~\ref{fig:franka_reach}. These results validate that the \textit{NeRD} model can effectively learn policies that transfer to the real world.

\begin{figure}[t!]
    \centering
    % \includegraphics[width=0.5 \linewidth]{figures/franka_exp_results.pdf}
    \includegraphics[width= 0.95\linewidth]{figures/franka_real_exp_v10.pdf}
    % \includegraphics[width= 0.95\linewidth]{figures/franka_real_exp_v11.pdf}
    
    \caption{\textbf{Zero-shot sim-to-real transfer of a Franka reach policy.} The real-world setup is shown in the \textbf{left} figure. The plot on the \textbf{middle} visualizes the evolution of distance-to-goal measurements when $10$ \textit{NeRD}-trained policies are executed, with a zoomed-in plot in the \textbf{right}.}
    \label{fig:franka_reach}
    % \vspace{-1em}
\end{figure}

\subsection{Fine-tunability on Real-World Data: Cube Tossing}
\label{sec:exp-cube-tossing}
\begin{figure*}[t!]
    \centering
    % \includegraphics[height=0.2\linewidth,trim=500pt 200pt 500pt 200pt,clip]{figures/cubetoss_warp.jpg}
    % \includegraphics[height=0.2\linewidth,trim=500pt 200pt 500pt 200pt,clip]{figures/cubetoss_finetune.jpg}
    % \includegraphics[width=\linewidth]{figures/cubetoss_exp_v2.pdf}
    \includegraphics[width=\linewidth]{figures/cubetoss_exp_v3.pdf}
    
    \caption{\textbf{Fine-tuning of a pretrained \textit{NeRD} model on real-world cube-tossing data.} \textbf{(a-b)} Cube-tossing trajectories simulated by the Warp simulator and by the fine-tuned \textit{NeRD} simulator. The light-green frames are ground-truth poses. \textbf{(c)} Comparison of fine-tuning a pretrained \textit{NeRD} model (red) against training a \textit{NeRD} model from scratch (blue) on the real dataset.}
    % \todo{A side-by-side figure for cube tossing results: one side states-time plots, another side visual overlay comparison between cube toss neural sim vs real data at different time steps}
    % Left: evaluation of simulated cube tossing environment against real cube tossing trajectory. Right: trajectory from the finetuned neural model.}
    \label{fig:cube_toss_comparison}
    \vspace{-1em}
\end{figure*}
We evaluate \textit{NeRD}'s fine-tunability using a real-world cube-tossing dataset \cite{pfrommer2021contactnets}, where a cube is tossed with a random initial state and collides with the ground. We first replicate this cube-tossing environment
%, to the best of our ability, 
in the Warp simulator and generate a synthetic dataset of cube-tossing trajectories for pretraining a \textit{NeRD} model. After pretraining, we fine-tune the \textit{NeRD} model on the real-world dataset. As a comparison, we also train a \textit{NeRD} model from scratch using only the real-world cube-tossing dataset (\ie no simulation data). We provide more details in the Appendix~\ref{sup:cube_toss}. 

We evaluate trained models on $85$ held-out real-world trajectories, and measure the average cube COM position error and orientation error along the trajectory. 
% As a reference for the position error, the side length of the cube is $0.1048$ \textit{m}. 
Since the \textbf{Warp simulator} does not fully capture real-world dynamics, it has a position and orientation error of $0.036$ \textit{m} and $0.383$ \textit{rad}, respectively. Both the fine-tuned \textit{NeRD} model and the \textit{NeRD} model trained from scratch outperform Warp. 
% in capturing real-world dynamics. 
Specifically, the \textbf{fine-tuned} \textit{NeRD} model has errors of $0.018$ \textit{m} and $0.266$ \textit{rad}, and the \textbf{model trained from scratch} has errors of $0.023$ \textit{m} and $0.276$ \textit{rad}. Fig. \ref{fig:cube_toss_comparison}(a) and (b) qualitatively compare the trajectories generated by Warp and the fine-tuned \textit{NeRD} model. 
% The visualization highlights the significant accuracy improvement achieved through fine-tuning with real-world data. 
In addition, Fig. \ref{fig:cube_toss_comparison}(c) shows that fine-tuning the pretrained \textit{NeRD} converges in fewer than five training epochs, which is $\bm{10\times}$ \textbf{faster} than training from scratch; thus, pretraining the \textit{NeRD} model on a large-scale simulation dataset enables efficient adaptation to real-world dynamics with a small amount of real-world data.

We further evaluate two baselines designed specifically for this dataset: (1) \textit{GNN-Rigid} \cite{allen2023contact} and (2) \textit{ContactNets} \cite{pfrommer2021contactnets}. For \textit{GNN-Rigid}, we used the 
released model and inference code (training code unavailable). The measured position error is $0.032$ \textit{m}; rotation error is not measured due to missing code. For \textit{ContactNets}, we used the released code to train the model. The evaluated position error is $0.017$ \textit{m} and rotation error is $0.242$ \textit{rad}. These comparisons show that \textit{NeRD} achieves comparable real-world fine-tuning results to specialized models while offering key advantages:  (1) \textit{NeRD} is widely applicable to diverse systems, including articulated and single rigid bodies; (2) Fine-tuning \textit{NeRD} took $<\!10$ min for the \textit{Cube Tossing} dataset, compared to $12$ h for \textit{ContactNets}.

% \subsection{Ablation Study on Design Decisions}
% \todo{We may need to move this part to Appendix.}

\label{sec:exp-ablation}