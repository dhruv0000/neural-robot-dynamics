{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruv0000/neural-robot-dynamics/blob/main/train_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B8R6x04hd4Ei",
      "metadata": {
        "id": "B8R6x04hd4Ei"
      },
      "source": [
        "# Neural Robot Dynamics Training on Colab\n",
        "\n",
        "This notebook demonstrates how to setup the environment, generate a dataset, and train the NeRD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "feb351a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feb351a0",
        "outputId": "eecce9f3-09df-449e-b0f1-a36e6c07eaa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neural-robot-dynamics'...\n",
            "remote: Enumerating objects: 457, done.\u001b[K\n",
            "remote: Counting objects: 100% (457/457), done.\u001b[K\n",
            "remote: Compressing objects: 100% (334/334), done.\u001b[K\n",
            "remote: Total 457 (delta 110), reused 425 (delta 79), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (457/457), 17.29 MiB | 17.49 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n",
            "Filtering content: 100% (11/11), 202.03 MiB | 78.45 MiB/s, done.\n",
            "/content/neural-robot-dynamics/train/neural-robot-dynamics\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting pyglet==2.1.6 (from -r requirements.txt (line 2))\n",
            "  Using cached pyglet-2.1.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ipdb (from -r requirements.txt (line 3))\n",
            "  Using cached ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting h5py==3.11.0 (from -r requirements.txt (line 4))\n",
            "  Using cached h5py-3.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pyyaml==6.0.2 (from -r requirements.txt (line 5))\n",
            "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting tensorboard==2.14.0 (from -r requirements.txt (line 6))\n",
            "  Using cached tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting matplotlib==3.7.5 (from -r requirements.txt (line 7))\n",
            "  Using cached matplotlib-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.12.0.88)\n",
            "Collecting pycollada==0.9.2 (from -r requirements.txt (line 9))\n",
            "  Using cached pycollada-0.9.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.10.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.10.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting warp-lang==1.8.0\n",
            "  Using cached warp_lang-1.8.0-py3-none-manylinux_2_28_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from warp-lang==1.8.0) (2.0.2)\n",
            "Downloading warp_lang-1.8.0-py3-none-manylinux_2_28_x86_64.whl (129.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: warp-lang\n",
            "Successfully installed warp-lang-1.8.0\n",
            "Requirement already satisfied: rl_games in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.12/dist-packages (from rl_games) (0.25.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.0.2)\n",
            "Requirement already satisfied: ray>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.52.0)\n",
            "Requirement already satisfied: tensorboard>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.19.0)\n",
            "Requirement already satisfied: tensorboardX>=1.6 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.6.4)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from rl_games) (1.3.7)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from rl_games) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from rl_games) (6.0.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->rl_games) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->rl_games) (0.1.0)\n",
            "Requirement already satisfied: click!=8.3.*,>=7.0 in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (3.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=1.14.0->rl_games) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.0->rl_games) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=1.14.0->rl_games) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (0.29.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup Environment\n",
        "!git clone https://github.com/dhruv0000/neural-robot-dynamics.git\n",
        "%cd neural-robot-dynamics\n",
        "!pip install -r requirements.txt\n",
        "!pip install warp-lang\n",
        "!pip install rl_games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "66700684",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66700684",
        "outputId": "2c88b345-5b03-4529-c259-e97c91dd8908"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neural-robot-dynamics/train/neural-robot-dynamics/generate\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 64 environments: 100% 64/64 [00:00<00:00, 198.69it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 21390.81 ms  (compiled)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 550.49 ms  (compiled)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 758.75 ms  (compiled)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "  0% 0/10000 [00:00<?, ?it/s]Module utils.warp_utils 294c46a load on device 'cuda:0' took 654.62 ms  (compiled)\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 16015.36 ms  (compiled)\n",
            "12800it [00:17, 861.25it/s]              \n",
            "\n",
            "Total number of transitions generated: 12800\n",
            "12800it [00:17, 715.68it/s]\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 64 environments: 100% 64/64 [00:00<00:00, 204.44it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 3.37 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.43 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.45 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "  0% 0/2000 [00:00<?, ?it/s]Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.50 ms  (cached)\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.37 ms  (cached)\n",
            "6400it [00:00, 14239.43it/s]\n",
            "\n",
            "Total number of transitions generated: 6400\n",
            "6400it [00:00, 14206.24it/s]\n",
            "/content/neural-robot-dynamics/train/neural-robot-dynamics\n"
          ]
        }
      ],
      "source": [
        "# 2. Generate Dataset\n",
        "# We generate a smaller dataset for demonstration purposes.\n",
        "\n",
        "%cd generate\n",
        "\n",
        "# Generate Training Data\n",
        "!python generate_dataset_contact_free.py --env-name Cartpole --num-transitions 10000 --dataset-dir ../data/datasets/ --dataset-name trajectory_len-100_train.hdf5 --trajectory-length 100 --num-envs 64 --seed 0\n",
        "\n",
        "# Generate Validation Data\n",
        "!python generate_dataset_contact_free.py --env-name Cartpole --num-transitions 2000 --dataset-dir ../data/datasets/ --dataset-name trajectory_len-100_valid.hdf5 --trajectory-length 100 --num-envs 64 --seed 10\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d0907b67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0907b67",
        "outputId": "966d2537-a2e9-4a5f-d8d4-c53e95b59fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neural-robot-dynamics/train/neural-robot-dynamics/train\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 22:43:51.544652: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763937831.563765    2581 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763937831.569606    2581 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763937831.584377    2581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937831.584400    2581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937831.584405    2581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937831.584410    2581 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:43:51.588993: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 326.74it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.17 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.44 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.51 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "number of parameters: 2.69M\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (transformer_model): GPT(\n",
            "    (transformer): ModuleDict(\n",
            "      (wte): Linear(in_features=6, out_features=192, bias=True)\n",
            "      (wpe): Embedding(32, 192)\n",
            "      (drop): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-5): 6 x Block(\n",
            "          (ln_1): LayerNorm()\n",
            "          (attn): CausalSelfAttention(\n",
            "            (c_attn): Linear(in_features=192, out_features=576, bias=False)\n",
            "            (c_proj): Linear(in_features=192, out_features=192, bias=False)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm()\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Linear(in_features=192, out_features=768, bias=False)\n",
            "            (gelu): GELU(approximate='none')\n",
            "            (c_proj): Linear(in_features=768, out_features=192, bias=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=192, out_features=192, bias=False)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  2713668\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:01<00:00,  8.56it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.46 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 975.89 ms  (compiled)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.33 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 3127.52 ms  (compiled)\n",
            "Sampling state transitions: 100% 4/4 [00:04<00:00,  1.10s/it]\n",
            "100% 4/4 [00:00<00:00,  5.47it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.57209951, Rollout MSE Error (joint_q) = 0.01352453 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 0.572099506855011. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.89327049, itemized = {state_0: 0.00767032, state_1: 0.87654409, state_2: 0.24498109, state_3: 0.27191060, state_MSE: 0.35027654, q_error_norm: 0.28657785, qd_error_norm: 0.52501436} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 6.483 sec, time(other): 0.001 sec, time(dataloader): 0.633 sec, time(compute_loss): 0.651 sec, time(backward): 0.000 sec, time(eval): 5.177 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 0.89327049. \u001b[0m\n",
            "100% 100/100 [00:09<00:00, 10.23it/s]\n",
            "100% 11/11 [00:00<00:00, 12.45it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 26.78it/s]\n",
            "100% 4/4 [00:00<00:00,  6.92it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.23402007, Rollout MSE Error (joint_q) = 0.00663242 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 1 with MSE error 0.23402006924152374. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.33126404, itemized = {state_0: 0.00083377, state_1: 0.23053471, state_2: 0.10704877, state_3: 0.21781916, state_MSE: 0.13905916, q_error_norm: 0.07858131, qd_error_norm: 0.37626648} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.14883819, itemized = {state_0: 0.00019616, state_1: 0.22869390, state_2: 0.04450290, state_3: 0.10678220, state_MSE: 0.09504379, q_error_norm: 0.06703554, qd_error_norm: 0.27306598} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.639 sec, time(other): 0.034 sec, time(dataloader): 5.145 sec, time(compute_loss): 2.305 sec, time(backward): 3.121 sec, time(eval): 0.777 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.018429, device='cuda:0'), grad_norm_after_clip: tensor(0.868848, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 0.14883819. \u001b[0m\n",
            "100% 100/100 [00:10<00:00,  9.76it/s]\n",
            "100% 11/11 [00:00<00:00, 12.36it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 21.85it/s]\n",
            "100% 4/4 [00:00<00:00,  6.91it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.07920126, Rollout MSE Error (joint_q) = 0.00158553 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 2 with MSE error 0.07920125871896744. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.08601153, itemized = {state_0: 0.00013337, state_1: 0.09280305, state_2: 0.03220012, state_3: 0.05785245, state_MSE: 0.04574726, q_error_norm: 0.03276653, qd_error_norm: 0.20147537} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.04793230, itemized = {state_0: 0.00007494, state_1: 0.06916192, state_2: 0.01865852, state_3: 0.03166708, state_MSE: 0.02989062, q_error_norm: 0.02387684, qd_error_norm: 0.14859157} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 12.153 sec, time(other): 0.019 sec, time(dataloader): 5.881 sec, time(compute_loss): 2.307 sec, time(backward): 2.901 sec, time(eval): 0.810 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.001073, device='cuda:0'), grad_norm_after_clip: tensor(0.918163, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 2 with loss 0.04793230. \u001b[0m\n",
            "100% 100/100 [00:10<00:00,  9.82it/s]\n",
            "100% 11/11 [00:00<00:00, 13.37it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.63it/s]\n",
            "100% 4/4 [00:00<00:00,  6.90it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.02733320, Rollout MSE Error (joint_q) = 0.00781744 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 3 with MSE error 0.027333198115229607. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.03553375, itemized = {state_0: 0.00005818, state_1: 0.06277299, state_2: 0.01556550, state_3: 0.02121960, state_MSE: 0.02490407, q_error_norm: 0.02191646, qd_error_norm: 0.12800590} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.03533276, itemized = {state_0: 0.00013613, state_1: 0.05467754, state_2: 0.01084322, state_3: 0.02340182, state_MSE: 0.02226468, q_error_norm: 0.02453578, qd_error_norm: 0.12020665} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.959 sec, time(other): 0.016 sec, time(dataloader): 5.831 sec, time(compute_loss): 2.241 sec, time(backward): 2.876 sec, time(eval): 0.773 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.003369, device='cuda:0'), grad_norm_after_clip: tensor(0.844422, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 3 with loss 0.03533276. \u001b[0m\n",
            "100% 100/100 [00:10<00:00,  9.71it/s]\n",
            "100% 11/11 [00:00<00:00, 13.32it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 24.85it/s]\n",
            "100% 4/4 [00:00<00:00,  6.83it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.02023264, Rollout MSE Error (joint_q) = 0.00103619 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 4 with MSE error 0.02023264206945896. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.01786481, itemized = {state_0: 0.00003381, state_1: 0.04683751, state_2: 0.00745018, state_3: 0.01087055, state_MSE: 0.01629802, q_error_norm: 0.01609722, qd_error_norm: 0.09254391} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.01935512, itemized = {state_0: 0.00002691, state_1: 0.09435847, state_2: 0.00706209, state_3: 0.01284382, state_MSE: 0.02857282, q_error_norm: 0.02523972, qd_error_norm: 0.08912633} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 12.134 sec, time(other): 0.017 sec, time(dataloader): 6.114 sec, time(compute_loss): 2.284 sec, time(backward): 2.660 sec, time(eval): 0.800 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.893509, device='cuda:0'), grad_norm_after_clip: tensor(0.779324, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 4 with loss 0.01935512. \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 3. Train Baseline Model (Transformer)\n",
        "%cd train\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "# Load default config\n",
        "with open('cfg/Cartpole/transformer.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Override dataset paths to point to the generated data\n",
        "cfg['algorithm']['dataset']['train_dataset_path'] = '../data/datasets/Cartpole/trajectory_len-100_train.hdf5'\n",
        "cfg['algorithm']['dataset']['valid_datasets']['exp_trajectory'] = '../data/datasets/Cartpole/trajectory_len-100_valid.hdf5'\n",
        "\n",
        "# Reduce training parameters for quick demonstration\n",
        "cfg['algorithm']['num_epochs'] = 5\n",
        "cfg['algorithm']['num_iters_per_epoch'] = 100\n",
        "\n",
        "# Save the modified config\n",
        "with open('colab_config.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "# Run training\n",
        "!python train.py --cfg colab_config.yaml --logdir ../data/logs/baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c69c48d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c69c48d1",
        "outputId": "91ddf525-65c7-4e58-ddfc-b747ea24cf0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 22:45:08.473715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763937908.492860    3362 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763937908.498738    3362 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763937908.513662    3362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937908.513690    3362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937908.513695    3362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763937908.513699    3362 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:45:08.518287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 317.24it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.38 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.47 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.65 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (mamba_model): Mamba(\n",
            "    (embedding): Linear(in_features=6, out_features=192, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x MambaBlock(\n",
            "        (in_proj): Linear(in_features=192, out_features=768, bias=False)\n",
            "        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)\n",
            "        (x_proj): Linear(in_features=384, out_features=44, bias=False)\n",
            "        (dt_proj): Linear(in_features=12, out_features=384, bias=True)\n",
            "        (out_proj): Linear(in_features=384, out_features=192, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (norm_f): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  1523460\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:01<00:00,  6.70it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.51 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.39 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.39 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.72 ms  (cached)\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 20.39it/s]\n",
            "100% 4/4 [00:01<00:00,  2.49it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.43373597, Rollout MSE Error (joint_q) = 0.06781688 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 1.433735966682434. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 1.00439644, itemized = {state_0: 0.00870172, state_1: 0.91801999, state_2: 0.25972505, state_3: 0.32500295, state_MSE: 0.37786241, q_error_norm: 0.30157635, qd_error_norm: 0.58810208} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 3.500 sec, time(other): 0.001 sec, time(dataloader): 0.564 sec, time(compute_loss): 1.076 sec, time(backward): 0.000 sec, time(eval): 1.842 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 1.00439644. \u001b[0m\n",
            "100% 100/100 [00:48<00:00,  2.07it/s]\n",
            "100% 11/11 [00:01<00:00,  8.14it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.29it/s]\n",
            "100% 4/4 [00:01<00:00,  2.49it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.01975624, Rollout MSE Error (joint_q) = 0.00050369 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 1 with MSE error 0.019756242632865906. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.17899710, itemized = {state_0: 0.00046378, state_1: 0.13165365, state_2: 0.06653225, state_3: 0.10681804, state_MSE: 0.07636691, q_error_norm: 0.04521545, qd_error_norm: 0.23944275} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.02070835, itemized = {state_0: 0.00004104, state_1: 0.05182135, state_2: 0.00865172, state_3: 0.01206261, state_MSE: 0.01814418, q_error_norm: 0.01769918, qd_error_norm: 0.09432975} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 51.800 sec, time(other): 0.373 sec, time(dataloader): 18.148 sec, time(compute_loss): 7.553 sec, time(backward): 23.550 sec, time(eval): 1.779 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.366601, device='cuda:0'), grad_norm_after_clip: tensor(0.351471, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 0.02070835. \u001b[0m\n",
            "100% 100/100 [00:48<00:00,  2.06it/s]\n",
            "100% 11/11 [00:01<00:00,  8.19it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.30it/s]\n",
            "100% 4/4 [00:01<00:00,  2.40it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00756090, Rollout MSE Error (joint_q) = 0.00031875 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 2 with MSE error 0.00756090460345149. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00893516, itemized = {state_0: 0.00002393, state_1: 0.04518652, state_2: 0.00386501, state_3: 0.00466609, state_MSE: 0.01343539, q_error_norm: 0.01450096, qd_error_norm: 0.06293787} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00797624, itemized = {state_0: 0.00001570, state_1: 0.06016227, state_2: 0.00303988, state_3: 0.00494928, state_MSE: 0.01704178, q_error_norm: 0.01584079, qd_error_norm: 0.05500800} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 52.093 sec, time(other): 0.372 sec, time(dataloader): 18.503 sec, time(compute_loss): 7.608 sec, time(backward): 23.345 sec, time(eval): 1.854 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.155297, device='cuda:0'), grad_norm_after_clip: tensor(0.155297, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 2 with loss 0.00797624. \u001b[0m\n",
            "100% 100/100 [00:49<00:00,  2.02it/s]\n",
            "100% 11/11 [00:01<00:00,  6.78it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 24.39it/s]\n",
            "100% 4/4 [00:01<00:00,  2.49it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00233163, Rollout MSE Error (joint_q) = 0.00021908 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 3 with MSE error 0.0023316273000091314. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00328931, itemized = {state_0: 0.00001129, state_1: 0.02878337, state_2: 0.00129734, state_3: 0.00167763, state_MSE: 0.00794240, q_error_norm: 0.00989465, qd_error_norm: 0.03948760} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00542831, itemized = {state_0: 0.00001065, state_1: 0.05317619, state_2: 0.00172920, state_3: 0.00372670, state_MSE: 0.01466068, q_error_norm: 0.01371052, qd_error_norm: 0.03887138} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 53.283 sec, time(other): 0.370 sec, time(dataloader): 19.523 sec, time(compute_loss): 7.780 sec, time(backward): 23.340 sec, time(eval): 1.804 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.083606, device='cuda:0'), grad_norm_after_clip: tensor(0.083606, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 3 with loss 0.00542831. \u001b[0m\n",
            "100% 100/100 [00:49<00:00,  2.02it/s]\n",
            "100% 11/11 [00:01<00:00,  8.46it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 24.92it/s]\n",
            "100% 4/4 [00:01<00:00,  2.49it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00168614, Rollout MSE Error (joint_q) = 0.00012483 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 4 with MSE error 0.0016861445037648082. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00182711, itemized = {state_0: 0.00000754, state_1: 0.02463967, state_2: 0.00064641, state_3: 0.00091838, state_MSE: 0.00655300, q_error_norm: 0.00838156, qd_error_norm: 0.02976539} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00459591, itemized = {state_0: 0.00000805, state_1: 0.05528026, state_2: 0.00134581, state_3: 0.00333575, state_MSE: 0.01499247, q_error_norm: 0.01346235, qd_error_norm: 0.03372930} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 53.040 sec, time(other): 0.381 sec, time(dataloader): 19.481 sec, time(compute_loss): 7.634 sec, time(backward): 23.318 sec, time(eval): 1.797 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.043410, device='cuda:0'), grad_norm_after_clip: tensor(0.043410, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 4 with loss 0.00459591. \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 4. Train Mamba Model\n",
        "# We use the same config but add the --novelty mamba flag\n",
        "!python train.py --cfg colab_config.yaml --novelty mamba --logdir ../data/logs/mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a32b3a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a32b3a28",
        "outputId": "1329c301-603c-47af-a832-f6661dfafffe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 22:49:00.396747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763938140.415896    4776 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763938140.421748    4776 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763938140.436626    4776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938140.436666    4776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938140.436673    4776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938140.436676    4776 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:49:00.441090: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 320.69it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 5.18 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.46 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "number of parameters: 2.69M\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (transformer_model): GPT(\n",
            "    (transformer): ModuleDict(\n",
            "      (wte): Linear(in_features=6, out_features=192, bias=True)\n",
            "      (wpe): Embedding(32, 192)\n",
            "      (drop): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-5): 6 x Block(\n",
            "          (ln_1): LayerNorm()\n",
            "          (attn): CausalSelfAttention(\n",
            "            (c_attn): Linear(in_features=192, out_features=576, bias=False)\n",
            "            (c_proj): Linear(in_features=192, out_features=192, bias=False)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm()\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Linear(in_features=192, out_features=768, bias=False)\n",
            "            (gelu): GELU(approximate='none')\n",
            "            (c_proj): Linear(in_features=768, out_features=192, bias=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=192, out_features=192, bias=False)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  2713668\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:01<00:00,  5.72it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.51 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.35 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.38 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.69 ms  (cached)\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 19.02it/s]\n",
            "100% 4/4 [00:00<00:00,  6.90it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.57209951, Rollout MSE Error (joint_q) = 0.01352453 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 0.572099506855011. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 3.68339885, itemized = {state_MSE: 3.68339885} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 2.801 sec, time(other): 0.000 sec, time(dataloader): 0.654 sec, time(compute_loss): 1.269 sec, time(backward): 0.000 sec, time(eval): 0.855 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 3.68339885. \u001b[0m\n",
            "100% 100/100 [00:33<00:00,  2.97it/s]\n",
            "100% 11/11 [00:01<00:00,  6.24it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.78it/s]\n",
            "100% 4/4 [00:00<00:00,  6.66it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.21425354, Rollout MSE Error (joint_q) = 0.33249471 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.95338409, itemized = {state_MSE: 2.95338409} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.29789838, itemized = {state_MSE: 2.29789838} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 36.636 sec, time(other): 0.006 sec, time(dataloader): 4.326 sec, time(compute_loss): 10.176 sec, time(backward): 20.886 sec, time(eval): 0.756 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(9.414604, device='cuda:0'), grad_norm_after_clip: tensor(1.000000, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 2.29789838. \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.91it/s]\n",
            "100% 11/11 [00:01<00:00,  7.38it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.91it/s]\n",
            "100% 4/4 [00:00<00:00,  6.83it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 2.11347532, Rollout MSE Error (joint_q) = 1.35214865 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.64865326, itemized = {state_MSE: 2.64865326} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.62989443, itemized = {state_MSE: 2.62989443} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 37.030 sec, time(other): 0.006 sec, time(dataloader): 4.780 sec, time(compute_loss): 10.329 sec, time(backward): 20.692 sec, time(eval): 0.740 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(33.423061, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.86it/s]\n",
            "100% 11/11 [00:01<00:00,  7.24it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.91it/s]\n",
            "100% 4/4 [00:00<00:00,  6.73it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.32412612, Rollout MSE Error (joint_q) = 0.42347786 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.85362000, itemized = {state_MSE: 2.85362000} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.58044659, itemized = {state_MSE: 2.58044659} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 37.545 sec, time(other): 0.006 sec, time(dataloader): 4.946 sec, time(compute_loss): 10.628 sec, time(backward): 20.829 sec, time(eval): 0.753 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(220.961975, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.87it/s]\n",
            "100% 11/11 [00:01<00:00,  7.43it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.93it/s]\n",
            "100% 4/4 [00:00<00:00,  6.78it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 2.38235402, Rollout MSE Error (joint_q) = 1.49744999 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.92595856, itemized = {state_MSE: 2.92595856} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.81243844, itemized = {state_MSE: 2.81243844} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 37.412 sec, time(other): 0.006 sec, time(dataloader): 5.072 sec, time(compute_loss): 10.521 sec, time(backward): 20.676 sec, time(eval): 0.748 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(529.858093, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 5. Train Unroll Model\n",
        "# We use the same config but add the --novelty unroll flag\n",
        "!python train.py --cfg colab_config.yaml --novelty unroll --logdir ../data/logs/unroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "638f64f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638f64f6",
        "outputId": "9ebea1c6-b140-4e52-fa82-cb5b15840f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Baseline Model...\n",
            "Using model: ../data/logs/baseline/11-23-2025-22-43-58/nn/best_eval_model.pt\n",
            "Evaluating Mamba Model...\n",
            "Using model: ../data/logs/mamba/11-23-2025-22-45-14/nn/best_eval_model.pt\n",
            "Evaluating Unroll Model...\n",
            "Using model: ../data/logs/unroll/11-23-2025-22-49-06/nn/best_eval_model.pt\n"
          ]
        }
      ],
      "source": [
        "# 6. RL Evaluation\n",
        "# Evaluate the trained models using the pretrained RL policy\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "\n",
        "def find_latest_model(model_type):\n",
        "    base_log_dir = f'../data/logs/{model_type}'\n",
        "    if not os.path.exists(base_log_dir):\n",
        "        print(f'Log dir not found: {base_log_dir}')\n",
        "        return None\n",
        "    dirs = [d for d in glob.glob(os.path.join(base_log_dir, '*')) if os.path.isdir(d)]\n",
        "    if not dirs:\n",
        "        print(f'No logs found for {model_type}')\n",
        "        return None\n",
        "    latest_dir = sorted(dirs)[-1]\n",
        "    model_path = os.path.join(latest_dir, 'nn', 'best_eval_model.pt')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f'Model file not found: {model_path}')\n",
        "        return None\n",
        "    return model_path\n",
        "\n",
        "models = ['baseline', 'mamba', 'unroll']\n",
        "for model in models:\n",
        "    print(f'Evaluating {model.capitalize()} Model...')\n",
        "    model_path = find_latest_model(model)\n",
        "    if model_path:\n",
        "        print(f'Using model: {model_path}')\n",
        "\n",
        "        # Convert paths to absolute to avoid issues with subprocess cwd\n",
        "        abs_model_path = os.path.abspath(model_path)\n",
        "        abs_playback_path = os.path.abspath('../pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth')\n",
        "        abs_rl_cfg_path = os.path.abspath('../eval/eval_rl/cfg/Cartpole/cartpole.yaml')\n",
        "\n",
        "        cmd = [\n",
        "            'python', 'run_rl.py',\n",
        "            '--rl-cfg', abs_rl_cfg_path,\n",
        "            '--playback', abs_playback_path,\n",
        "            '--num-envs', '1',\n",
        "            '--num-games', '5',\n",
        "            '--env-mode', 'neural',\n",
        "            '--nerd-model-path', abs_model_path\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            subprocess.run(cmd, cwd='../eval/eval_rl', check=True, capture_output=True, text=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f'Error running RL evaluation for {model}:')\n",
        "            print('STDOUT:', e.stdout)\n",
        "            print('STDERR:', e.stderr)\n",
        "            raise e\n",
        "    else:\n",
        "        print(f'Skipping {model} evaluation.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "quantitative-analysis-header",
      "metadata": {
        "id": "quantitative-analysis-header"
      },
      "source": [
        "# 7. Quantitative Analysis\n",
        "\n",
        "We now perform the quantitative analysis as described in the paper experiments.\n",
        "We evaluate:\n",
        "1. **Long-Horizon Passive Motion**: Accuracy of the trained NeRD models over 100, 500, and 1000 steps.\n",
        "2. **RL Policy Evaluation**: Performance of the pretrained RL policy using the NeRD models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "passive-motion-eval",
      "metadata": {
        "id": "passive-motion-eval",
        "outputId": "e663a989-4c9e-4258-cbfb-9f548df3b120",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Evaluating Baseline Model ====================\n",
            "\n",
            "--- Horizon: 100 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 500 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 1000 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "==================== Evaluating Mamba Model ====================\n",
            "\n",
            "--- Horizon: 100 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 500 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 1000 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "==================== Evaluating Unroll Model ====================\n",
            "\n",
            "--- Horizon: 100 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 500 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
            "\n",
            "--- Horizon: 1000 ---\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/neural-robot-dynamics/train/neural-robot-dynamics/train/../eval/eval_passive/eval_passive_motion.py\", line 97, in <module>\n",
            "    model, robot_name = torch.load(args.model_path, map_location='cuda:0')\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1529, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL models.models.ModelMixedInput was not an allowed global by default. Please use `torch.serialization.add_safe_globals([models.models.ModelMixedInput])` or the `torch.serialization.safe_globals([models.models.ModelMixedInput])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ],
      "source": [
        "# 7.1 Long-Horizon Passive Motion Evaluation\n",
        "# We evaluate the Baseline, Mamba, and Unroll models on Cartpole for 100, 500, and 1000 steps.\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "def find_latest_model(model_type):\n",
        "    base_log_dir = f'../data/logs/{model_type}'\n",
        "    if not os.path.exists(base_log_dir):\n",
        "        return None\n",
        "    dirs = [d for d in glob.glob(os.path.join(base_log_dir, '*')) if os.path.isdir(d)]\n",
        "    if not dirs:\n",
        "        return None\n",
        "    latest_dir = sorted(dirs)[-1]\n",
        "    model_path = os.path.join(latest_dir, 'nn', 'best_eval_model.pt')\n",
        "    if not os.path.exists(model_path):\n",
        "        return None\n",
        "    return model_path\n",
        "\n",
        "models = ['baseline', 'mamba', 'unroll']\n",
        "horizons = [100, 500, 1000]\n",
        "\n",
        "for model_name in models:\n",
        "    model_path = find_latest_model(model_name)\n",
        "    if not model_path:\n",
        "        print(f\"Skipping {model_name} (model not found)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*20} Evaluating {model_name.capitalize()} Model {'='*20}\")\n",
        "    for horizon in horizons:\n",
        "        print(f\"\\n--- Horizon: {horizon} ---\")\n",
        "        # We use !python to ensure output is printed to the cell\n",
        "        !python ../eval/eval_passive/eval_passive_motion.py \\\n",
        "            --env-name Cartpole \\\n",
        "            --model-path {model_path} \\\n",
        "            --env-mode neural \\\n",
        "            --num-envs 2048 \\\n",
        "            --num-rollouts 2048 \\\n",
        "            --rollout-horizon {horizon} \\\n",
        "            --seed 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "rl-policy-eval-quant",
      "metadata": {
        "id": "rl-policy-eval-quant",
        "outputId": "ac0b3a81-c2a3-4f0b-bd35-b01fdb2cb387",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== RL Evaluation: Baseline Model ====================\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-11-23 22:55:03.005215: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763938503.024862    6862 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763938503.030778    6862 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763938503.045544    6862 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938503.045569    6862 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938503.045573    6862 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938503.045576    6862 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:55:03.050085: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 2048 environments: 100% 2048/2048 [00:07<00:00, 278.78it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 2.45 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.32 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.46 ms  (cached)\n",
            "\u001b[96m [NeuralEnvironment] Created a Neural Integrator. \u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "self.seed = 0\n",
            "Started to play\n",
            "{'observation_space': Box(-100.0, 100.0, (4,), float32), 'action_space': Box(-1.0, 1.0, (1,), float32), 'agents': 1, 'value_size': 1}\n",
            "build mlp: 4\n",
            "RunningMeanStd:  (1,)\n",
            "RunningMeanStd:  (4,)\n",
            "=> loading checkpoint '/content/neural-robot-dynamics/train/neural-robot-dynamics/pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth'\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.37 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.34 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.37 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.86 ms  (cached)\n",
            "Module envs.rlgames_env_wrapper 947f136 load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "Module envs.warp_sim_envs.wrapper_utils 37d6d77 load on device 'cuda:0' took 0.39 ms  (cached)\n",
            "reward: -9.682574272155762 steps: 22.0\n",
            "reward: -5.915550231933594 steps: 23.0\n",
            "reward: 1.7591114044189453 steps: 24.0\n",
            "reward: -18.887592315673828 steps: 25.0\n",
            "reward: -42.319698333740234 steps: 29.0\n",
            "reward: -31.741230010986328 steps: 33.0\n",
            "reward: -21.6249942779541 steps: 34.0\n",
            "reward: -8.399645487467447 steps: 35.0\n",
            "reward: -63.00756072998047 steps: 36.0\n",
            "reward: -44.2802734375 steps: 37.0\n",
            "reward: -23.631052652994793 steps: 38.0\n",
            "reward: -16.77834701538086 steps: 39.0\n",
            "reward: -11.422149658203125 steps: 40.0\n",
            "reward: -18.26143455505371 steps: 41.0\n",
            "reward: -153.65524291992188 steps: 42.0\n",
            "reward: -78.67572021484375 steps: 43.0\n",
            "reward: 8.086751937866211 steps: 45.0\n",
            "reward: -145.44882202148438 steps: 46.0\n",
            "reward: -29.67266337076823 steps: 47.0\n",
            "reward: 17.453882217407227 steps: 48.0\n",
            "reward: 77.44083404541016 steps: 49.0\n",
            "reward: -98.5651346842448 steps: 51.0\n",
            "reward: 14.376520792643229 steps: 53.0\n",
            "reward: 54.260005950927734 steps: 54.0\n",
            "reward: 88.61528778076172 steps: 56.0\n",
            "reward: 78.20263671875 steps: 57.0\n",
            "reward: 80.85944366455078 steps: 58.0\n",
            "reward: -76.02642822265625 steps: 59.0\n",
            "reward: -55.9331169128418 steps: 60.0\n",
            "reward: 77.61710357666016 steps: 63.0\n",
            "reward: 92.82780456542969 steps: 64.0\n",
            "reward: 52.032657623291016 steps: 48.5\n",
            "reward: -114.46669006347656 steps: 70.0\n",
            "reward: -17.98887888590495 steps: 72.0\n",
            "reward: -7.530867576599121 steps: 28.0\n",
            "reward: -160.9832305908203 steps: 94.0\n",
            "reward: -198.8271484375 steps: 106.0\n",
            "reward: -92.32470703125 steps: 107.0\n",
            "reward: -3.3499679565429688 steps: 113.0\n",
            "reward: -34.55275344848633 steps: 119.0\n",
            "reward: 80.45813751220703 steps: 127.0\n",
            "reward: 77.14029693603516 steps: 128.0\n",
            "reward: 110.61698913574219 steps: 134.0\n",
            "reward: 603.66455078125 steps: 170.0\n",
            "reward: 613.3776245117188 steps: 173.0\n",
            "reward: 622.5935668945312 steps: 179.0\n",
            "reward: 663.8951416015625 steps: 187.0\n",
            "reward: 796.1543579101562 steps: 221.0\n",
            "reward: 84.5901107788086 steps: 52.0\n",
            "reward: 585.1619873046875 steps: 244.0\n",
            "reward: 1083.851806640625 steps: 264.0\n",
            "reward: 1084.1263427734375 steps: 270.0\n",
            "reward: 1139.8433837890625 steps: 273.0\n",
            "reward: 1125.7255859375 steps: 274.0\n",
            "reward: 1144.9456787109375 steps: 277.0\n",
            "reward: 1161.110595703125 steps: 278.0\n",
            "reward: 1101.771240234375 steps: 279.0\n",
            "reward: 1157.58984375 steps: 280.0\n",
            "reward: 1164.9222412109375 steps: 281.0\n",
            "reward: 1180.3353271484375 steps: 283.0\n",
            "reward: 1194.6337890625 steps: 286.0\n",
            "reward: 1135.0640869140625 steps: 287.0\n",
            "reward: 1207.76708984375 steps: 289.0\n",
            "reward: 1218.4296875 steps: 291.0\n",
            "reward: 1219.31689453125 steps: 292.0\n",
            "reward: 1239.9969482421875 steps: 293.0\n",
            "reward: 1224.8743489583333 steps: 296.0\n",
            "reward: 1259.690673828125 steps: 297.0\n",
            "reward: 1267.3876953125 steps: 298.0\n",
            "reward: 1273.3040771484375 steps: 299.0\n",
            "reward: 1090.1022199277234 steps: 300.0\n",
            "2155489.165640831\n",
            "av reward: 1050.9454732524773 av steps: 290.6616284739152\n",
            "visited states range:\n",
            "State 0: [-4.109048366546631, 3.390089750289917]\n",
            "State 1: [-3.141591787338257, 3.141425609588623]\n",
            "State 2: [-10.443437576293945, 8.636869430541992]\n",
            "State 3: [-8.79551887512207, 9.134634971618652]\n",
            "\n",
            "==================== RL Evaluation: Mamba Model ====================\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-11-23 22:55:39.416824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763938539.436115    7034 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763938539.442320    7034 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763938539.456987    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938539.457013    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938539.457017    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938539.457021    7034 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:55:39.462377: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 2048 environments: 100% 2048/2048 [00:07<00:00, 279.13it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 3.57 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.31 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.44 ms  (cached)\n",
            "\u001b[96m [NeuralEnvironment] Created a Neural Integrator. \u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "self.seed = 0\n",
            "Started to play\n",
            "{'observation_space': Box(-100.0, 100.0, (4,), float32), 'action_space': Box(-1.0, 1.0, (1,), float32), 'agents': 1, 'value_size': 1}\n",
            "build mlp: 4\n",
            "RunningMeanStd:  (1,)\n",
            "RunningMeanStd:  (4,)\n",
            "=> loading checkpoint '/content/neural-robot-dynamics/train/neural-robot-dynamics/pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth'\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.75 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.46 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.52 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.83 ms  (cached)\n",
            "Module envs.rlgames_env_wrapper 947f136 load on device 'cuda:0' took 0.35 ms  (cached)\n",
            "Module envs.warp_sim_envs.wrapper_utils 37d6d77 load on device 'cuda:0' took 0.37 ms  (cached)\n",
            "reward: -125.61665344238281 steps: 24.0\n",
            "reward: -11.112984657287598 steps: 27.0\n",
            "reward: -21.08568000793457 steps: 33.0\n",
            "reward: -131.51220703125 steps: 37.0\n",
            "reward: -98.06793975830078 steps: 39.0\n",
            "reward: 10.90812873840332 steps: 40.0\n",
            "reward: -99.6205546061198 steps: 41.0\n",
            "reward: -92.2998758951823 steps: 42.0\n",
            "reward: -147.5164998372396 steps: 44.0\n",
            "reward: -157.23263549804688 steps: 45.0\n",
            "reward: -123.06836700439453 steps: 46.0\n",
            "reward: -141.76390075683594 steps: 47.0\n",
            "reward: 82.1233901977539 steps: 49.0\n",
            "reward: 32.15250015258789 steps: 52.0\n",
            "reward: 62.47734451293945 steps: 53.0\n",
            "reward: -46.76546096801758 steps: 55.0\n",
            "reward: 74.38868713378906 steps: 58.0\n",
            "reward: 116.78228759765625 steps: 59.0\n",
            "reward: 144.40359497070312 steps: 61.0\n",
            "reward: 108.03217315673828 steps: 63.0\n",
            "reward: 171.17343139648438 steps: 65.0\n",
            "reward: 183.58721923828125 steps: 67.0\n",
            "reward: 136.9559326171875 steps: 70.0\n",
            "reward: 208.64093017578125 steps: 71.0\n",
            "reward: 252.021240234375 steps: 82.0\n",
            "reward: 31.382410049438477 steps: 44.0\n",
            "reward: 278.45269775390625 steps: 90.0\n",
            "reward: 281.0409240722656 steps: 93.0\n",
            "reward: 149.35215759277344 steps: 98.0\n",
            "reward: 1112.8921275535624 steps: 300.0\n",
            "2235136.476559639\n",
            "av reward: 1090.8425947094383 av steps: 294.9853587115666\n",
            "visited states range:\n",
            "State 0: [-4.120481491088867, 4.022305488586426]\n",
            "State 1: [-3.1412806510925293, 3.141573905944824]\n",
            "State 2: [-10.158655166625977, 8.310407638549805]\n",
            "State 3: [-10.747008323669434, 9.119991302490234]\n",
            "\n",
            "==================== RL Evaluation: Unroll Model ====================\n",
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "2025-11-23 22:56:16.144939: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763938576.164449    7204 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763938576.170655    7204 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763938576.185470    7204 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938576.185497    7204 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938576.185501    7204 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763938576.185507    7204 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 22:56:16.189969: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 2048 environments: 100% 2048/2048 [00:07<00:00, 277.88it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 3.59 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.32 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.55 ms  (cached)\n",
            "\u001b[96m [NeuralEnvironment] Created a Neural Integrator. \u001b[0m\n",
            "/usr/local/lib/python3.12/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
            "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
            "self.seed = 0\n",
            "Started to play\n",
            "{'observation_space': Box(-100.0, 100.0, (4,), float32), 'action_space': Box(-1.0, 1.0, (1,), float32), 'agents': 1, 'value_size': 1}\n",
            "build mlp: 4\n",
            "RunningMeanStd:  (1,)\n",
            "RunningMeanStd:  (4,)\n",
            "=> loading checkpoint '/content/neural-robot-dynamics/train/neural-robot-dynamics/pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth'\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.49 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.36 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.37 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.86 ms  (cached)\n",
            "Module envs.rlgames_env_wrapper 947f136 load on device 'cuda:0' took 0.43 ms  (cached)\n",
            "Module envs.warp_sim_envs.wrapper_utils 37d6d77 load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "reward: -441.9017333984375 steps: 138.0\n",
            "reward: -518.52783203125 steps: 140.0\n",
            "reward: -539.6917114257812 steps: 141.0\n",
            "reward: -571.6729736328125 steps: 142.0\n",
            "reward: -568.0729370117188 steps: 143.0\n",
            "reward: -652.4376831054688 steps: 144.0\n",
            "reward: -543.55224609375 steps: 145.0\n",
            "reward: -527.66865234375 steps: 146.0\n",
            "reward: -608.8748779296875 steps: 147.0\n",
            "reward: -604.1287434895834 steps: 148.0\n",
            "reward: -648.0216238839286 steps: 149.0\n",
            "reward: -618.1834106445312 steps: 150.0\n",
            "reward: -644.165283203125 steps: 151.0\n",
            "reward: -662.3899739583334 steps: 152.0\n",
            "reward: -640.4921875 steps: 153.0\n",
            "reward: -689.3615056818181 steps: 154.0\n",
            "reward: -676.6314453125 steps: 155.0\n",
            "reward: -606.5030924479166 steps: 156.0\n",
            "reward: -641.9283854166666 steps: 157.0\n",
            "reward: -642.9236061789773 steps: 158.0\n",
            "reward: -666.5461282169117 steps: 159.0\n",
            "reward: -674.96240234375 steps: 160.0\n",
            "reward: -660.9192116477273 steps: 161.0\n",
            "reward: -598.5480769230769 steps: 162.0\n",
            "reward: -645.5125450721154 steps: 163.0\n",
            "reward: -639.7771183894231 steps: 164.0\n",
            "reward: -659.8205217633929 steps: 165.0\n",
            "reward: -648.154296875 steps: 166.0\n",
            "reward: -670.2994384765625 steps: 167.0\n",
            "reward: -647.6939453125 steps: 168.0\n",
            "reward: -611.6415608723959 steps: 169.0\n",
            "reward: -636.971630859375 steps: 170.0\n",
            "reward: -630.75732421875 steps: 171.0\n",
            "reward: -597.2899169921875 steps: 172.0\n",
            "reward: -601.4822048611111 steps: 173.0\n",
            "reward: -574.0912679036459 steps: 174.0\n",
            "reward: -616.0714192708333 steps: 175.0\n",
            "reward: -612.2993539663462 steps: 176.0\n",
            "reward: -598.7735072544643 steps: 177.0\n",
            "reward: -601.86162109375 steps: 178.0\n",
            "reward: -611.8932834201389 steps: 179.0\n",
            "reward: -594.9722493489584 steps: 180.0\n",
            "reward: -605.84130859375 steps: 181.0\n",
            "reward: -630.8155517578125 steps: 182.0\n",
            "reward: -583.93876953125 steps: 183.0\n",
            "reward: -608.2246704101562 steps: 184.0\n",
            "reward: -611.1850210336538 steps: 185.0\n",
            "reward: -624.4009232954545 steps: 186.0\n",
            "reward: -641.7086046006945 steps: 187.0\n",
            "reward: -616.439892578125 steps: 188.0\n",
            "reward: -536.9554268973214 steps: 189.0\n",
            "reward: -343.8974880642361 steps: 190.0\n",
            "reward: -622.25390625 steps: 191.0\n",
            "reward: -385.3331298828125 steps: 192.0\n",
            "reward: -367.01567925347223 steps: 193.0\n",
            "reward: -485.371484375 steps: 194.0\n",
            "reward: -297.39369419642856 steps: 195.0\n",
            "reward: -382.4760044642857 steps: 196.0\n",
            "reward: -315.1245849609375 steps: 197.0\n",
            "reward: -559.7495465959821 steps: 198.0\n",
            "reward: -197.05788845486111 steps: 199.0\n",
            "reward: -278.85342684659093 steps: 200.0\n",
            "reward: -359.6614583333333 steps: 201.0\n",
            "reward: -415.73475864955356 steps: 202.0\n",
            "reward: -374.405517578125 steps: 203.0\n",
            "reward: -300.7891845703125 steps: 204.0\n",
            "reward: -258.1523960658482 steps: 205.0\n",
            "reward: -152.48127092633928 steps: 206.0\n",
            "reward: -332.079345703125 steps: 207.0\n",
            "reward: -158.45263671875 steps: 208.0\n",
            "reward: -77.27968462775735 steps: 209.0\n",
            "reward: -33.402862548828125 steps: 210.0\n",
            "reward: -304.89625 steps: 211.0\n",
            "reward: -105.75362998560855 steps: 212.0\n",
            "reward: -12.374064569887908 steps: 213.0\n",
            "reward: -154.2697509765625 steps: 214.0\n",
            "reward: -231.28776041666666 steps: 215.0\n",
            "reward: -193.671142578125 steps: 216.0\n",
            "reward: -205.204736328125 steps: 217.0\n",
            "reward: -188.6778361002604 steps: 218.0\n",
            "reward: -129.37765984786185 steps: 219.0\n",
            "reward: -104.3064453125 steps: 220.0\n",
            "reward: -108.71669358473558 steps: 221.0\n",
            "reward: -96.522216796875 steps: 222.0\n",
            "reward: -84.1987813313802 steps: 223.0\n",
            "reward: -111.84510633680556 steps: 224.0\n",
            "reward: -86.21017456054688 steps: 225.0\n",
            "reward: -171.0104391163793 steps: 226.0\n",
            "reward: -114.81802528782895 steps: 227.0\n",
            "reward: -73.27882603236607 steps: 228.0\n",
            "reward: -163.6451416015625 steps: 229.0\n",
            "reward: -197.8330720600329 steps: 230.0\n",
            "reward: -70.3057861328125 steps: 231.0\n",
            "reward: -229.41824001736111 steps: 232.0\n",
            "reward: -13.044176737467447 steps: 233.0\n",
            "reward: -63.58572208180147 steps: 234.0\n",
            "reward: -50.9231591796875 steps: 235.0\n",
            "reward: -70.69114854600694 steps: 236.0\n",
            "reward: -225.36746215820312 steps: 237.0\n",
            "reward: -152.36881938733552 steps: 238.0\n",
            "reward: -178.3339201274671 steps: 239.0\n",
            "reward: -187.3368422564338 steps: 240.0\n",
            "reward: -143.59883939302884 steps: 241.0\n",
            "reward: -106.37255859375 steps: 242.0\n",
            "reward: -21.700183868408203 steps: 243.0\n",
            "reward: -191.97874620225696 steps: 244.0\n",
            "reward: -103.11288631663604 steps: 245.0\n",
            "reward: -180.24323918269232 steps: 246.0\n",
            "reward: 15.6035400390625 steps: 247.0\n",
            "reward: 24.299940842848557 steps: 248.0\n",
            "reward: -39.21473000266335 steps: 249.0\n",
            "reward: -68.58342895507812 steps: 250.0\n",
            "reward: -54.95966254340278 steps: 251.0\n",
            "reward: -135.348876953125 steps: 252.0\n",
            "reward: -31.72723642985026 steps: 253.0\n",
            "reward: -158.62638346354166 steps: 254.0\n",
            "reward: -80.48993682861328 steps: 255.0\n",
            "reward: -56.363945456112134 steps: 256.0\n",
            "reward: -34.616845703125 steps: 257.0\n",
            "reward: -202.55685221354167 steps: 258.0\n",
            "reward: -72.70087224786931 steps: 259.0\n",
            "reward: -40.9503173828125 steps: 260.0\n",
            "reward: -78.51303609212239 steps: 261.0\n",
            "reward: 24.637767246791295 steps: 262.0\n",
            "reward: -14.409477233886719 steps: 263.0\n",
            "reward: -28.03363741361178 steps: 264.0\n",
            "reward: 78.68324973366477 steps: 265.0\n",
            "reward: -56.1286735534668 steps: 266.0\n",
            "reward: -39.37049696180556 steps: 267.0\n",
            "reward: 100.17427571614583 steps: 268.0\n",
            "reward: 195.2856201171875 steps: 269.0\n",
            "reward: -56.91682434082031 steps: 270.0\n",
            "reward: 63.45014572143555 steps: 271.0\n",
            "reward: 0.752685546875 steps: 272.0\n",
            "reward: 216.627099609375 steps: 273.0\n",
            "reward: 81.42515055338542 steps: 274.0\n",
            "reward: 103.5791015625 steps: 275.0\n",
            "reward: 68.43767547607422 steps: 276.0\n",
            "reward: 36.583883666992186 steps: 277.0\n",
            "reward: 207.341064453125 steps: 278.0\n",
            "reward: 123.6893798828125 steps: 279.0\n",
            "reward: 145.64701625279017 steps: 280.0\n",
            "reward: 183.31883239746094 steps: 281.0\n",
            "reward: -24.014157104492188 steps: 282.0\n",
            "reward: 225.93720354352678 steps: 283.0\n",
            "reward: 212.5287109375 steps: 284.0\n",
            "reward: -178.60575358072916 steps: 285.0\n",
            "reward: 46.42060852050781 steps: 286.0\n",
            "reward: 120.25340053013393 steps: 287.0\n",
            "reward: 40.24851989746094 steps: 288.0\n",
            "reward: 123.38894314236111 steps: 289.0\n",
            "reward: -93.11602020263672 steps: 290.0\n",
            "reward: 143.227880859375 steps: 291.0\n",
            "reward: 217.16004943847656 steps: 292.0\n",
            "reward: 193.35652669270834 steps: 293.0\n",
            "reward: 75.90689086914062 steps: 294.0\n",
            "reward: 52.76202392578125 steps: 295.0\n",
            "reward: -43.59581298828125 steps: 296.0\n",
            "reward: 185.62431989397322 steps: 297.0\n",
            "reward: 169.7912139892578 steps: 298.0\n",
            "reward: 8.71957524617513 steps: 299.0\n",
            "reward: 187.68758461191337 steps: 299.4620938628159\n",
            "-392548.7232208252\n",
            "av reward: -191.58063602773314 av steps: 228.88823816495852\n",
            "visited states range:\n",
            "State 0: [-0.9902818202972412, 3.041212558746338]\n",
            "State 1: [-3.141587495803833, 3.1415855884552]\n",
            "State 2: [-1.5647469758987427, 10.074750900268555]\n",
            "State 3: [-3.584223985671997, 1.2496743202209473]\n"
          ]
        }
      ],
      "source": [
        "# 7.2 RL Policy Evaluation (Quantitative)\n",
        "# We evaluate the policy using the trained NeRD models.\n",
        "# We run for more games (2048) to get a statistically significant result as in the paper.\n",
        "\n",
        "for model_name in models:\n",
        "    model_path = find_latest_model(model_name)\n",
        "    if not model_path:\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n{'='*20} RL Evaluation: {model_name.capitalize()} Model {'='*20}\")\n",
        "\n",
        "    # Absolute paths for safety\n",
        "    abs_model_path = os.path.abspath(model_path)\n",
        "    abs_playback_path = os.path.abspath('../pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth')\n",
        "    abs_rl_cfg_path = os.path.abspath('../eval/eval_rl/cfg/Cartpole/cartpole.yaml')\n",
        "\n",
        "    # Run RL evaluation\n",
        "    !python ../eval/eval_rl/run_rl.py \\\n",
        "        --rl-cfg {abs_rl_cfg_path} \\\n",
        "        --playback {abs_playback_path} \\\n",
        "        --num-envs 2048 \\\n",
        "        --num-games 2048 \\\n",
        "        --env-mode neural \\\n",
        "        --nerd-model-path {abs_model_path}"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}