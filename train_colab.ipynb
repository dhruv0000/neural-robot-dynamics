{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a262a578",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhruv0000/neural-robot-dynamics/blob/main/train_colab.ipynb%20latest%20run\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B8R6x04hd4Ei",
      "metadata": {
        "id": "B8R6x04hd4Ei"
      },
      "source": [
        "# Neural Robot Dynamics Training on Colab\n",
        "\n",
        "This notebook demonstrates how to setup the environment, generate a dataset, and train the NeRD model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "feb351a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feb351a0",
        "outputId": "a36132fe-a43a-4068-9d8b-357fabdffd16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'neural-robot-dynamics'...\n",
            "remote: Enumerating objects: 444, done.\u001b[K\n",
            "remote: Counting objects: 100% (444/444), done.\u001b[K\n",
            "remote: Compressing objects: 100% (326/326), done.\u001b[K\n",
            "remote: Total 444 (delta 101), reused 416 (delta 74), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (444/444), 17.28 MiB | 18.68 MiB/s, done.\n",
            "Resolving deltas: 100% (101/101), done.\n",
            "Filtering content: 100% (11/11), 202.03 MiB | 71.46 MiB/s, done.\n",
            "/content/neural-robot-dynamics\n",
            "Requirement already satisfied: tqdm==4.67.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (4.67.1)\n",
            "Collecting pyglet==2.1.6 (from -r requirements.txt (line 2))\n",
            "  Downloading pyglet-2.1.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting ipdb (from -r requirements.txt (line 3))\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting h5py==3.11.0 (from -r requirements.txt (line 4))\n",
            "  Downloading h5py-3.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting pyyaml==6.0.2 (from -r requirements.txt (line 5))\n",
            "  Downloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting tensorboard==2.14.0 (from -r requirements.txt (line 6))\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting matplotlib==3.7.5 (from -r requirements.txt (line 7))\n",
            "  Downloading matplotlib-3.7.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 8)) (4.12.0.88)\n",
            "Collecting pycollada==0.9.2 (from -r requirements.txt (line 9))\n",
            "  Downloading pycollada-0.9.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 1.11.0, 1.14.0rc1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 1.10.0 Requires-Python <3.12,>=3.8; 1.10.0rc1 Requires-Python <3.12,>=3.8; 1.10.0rc2 Requires-Python <3.12,>=3.8; 1.10.1 Requires-Python <3.12,>=3.8; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.0 Requires-Python >=3.8,<3.12; 1.9.0rc1 Requires-Python >=3.8,<3.12; 1.9.0rc2 Requires-Python >=3.8,<3.12; 1.9.0rc3 Requires-Python >=3.8,<3.12; 1.9.1 Requires-Python >=3.8,<3.12\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement scipy==1.10.1 (from versions: 0.8.0, 0.9.0, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.12.1, 0.13.0, 0.13.1, 0.13.2, 0.13.3, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.16.0, 0.16.1, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.6.0, 1.6.1, 1.9.2, 1.9.3, 1.11.0rc1, 1.11.0rc2, 1.11.1, 1.11.2, 1.11.3, 1.11.4, 1.12.0rc1, 1.12.0rc2, 1.12.0, 1.13.0rc1, 1.13.0, 1.13.1, 1.14.0rc2, 1.14.0, 1.14.1, 1.15.0rc1, 1.15.0rc2, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.16.0rc1, 1.16.0rc2, 1.16.0, 1.16.1, 1.16.2, 1.16.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for scipy==1.10.1\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting warp-lang==1.8.0\n",
            "  Downloading warp_lang-1.8.0-py3-none-manylinux_2_28_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from warp-lang==1.8.0) (2.0.2)\n",
            "Downloading warp_lang-1.8.0-py3-none-manylinux_2_28_x86_64.whl (129.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: warp-lang\n",
            "Successfully installed warp-lang-1.8.0\n",
            "Collecting rl_games\n",
            "  Downloading rl-games-1.6.1.tar.gz (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML<7.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (6.0.3)\n",
            "Collecting gym<0.24.0,>=0.23.0 (from gym[classic-control]<0.24.0,>=0.23.0->rl_games)\n",
            "  Downloading gym-0.23.1.tar.gz (626 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: opencv-python<5.0.0,>=4.5.5 in /usr/local/lib/python3.12/dist-packages (from rl_games) (4.12.0.88)\n",
            "Requirement already satisfied: psutil<6.0.0,>=5.9.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (5.9.5)\n",
            "Collecting setproctitle<2.0.0,>=1.2.2 (from rl_games)\n",
            "  Downloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tensorboard<3.0.0,>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.19.0)\n",
            "Collecting tensorboardX<3.0,>=2.5 (from rl_games)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting wandb<0.13.0,>=0.12.11 (from rl_games)\n",
            "  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "INFO: pip is looking at multiple versions of rl-games to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting rl_games\n",
            "  Downloading rl-games-1.6.0.tar.gz (14.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading rl_games-1.5.2-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.12/dist-packages (from rl_games) (0.25.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from rl_games) (2.0.2)\n",
            "Collecting ray>=1.1.0 (from rl_games)\n",
            "  Downloading ray-2.52.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->rl_games) (3.1.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.12/dist-packages (from gym>=0.17.2->rl_games) (0.1.0)\n",
            "Collecting click!=8.3.*,>=7.0 (from ray>=1.1.0->rl_games)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (3.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (25.0)\n",
            "Requirement already satisfied: protobuf>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ray>=1.1.0->rl_games) (2.32.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (3.10)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3.0.0,>=2.8.0->rl_games) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.7.0->rl_games) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.7.0->rl_games) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard<3.0.0,>=2.8.0->rl_games) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray>=1.1.0->rl_games) (0.29.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ray>=1.1.0->rl_games) (2025.11.12)\n",
            "Downloading rl_games-1.5.2-py3-none-any.whl (15.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.52.0-cp312-cp312-manylinux2014_x86_64.whl (72.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.3/72.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.7-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (32 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorboardX, setproctitle, click, ray, rl_games\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.3.1\n",
            "    Uninstalling click-8.3.1:\n",
            "      Successfully uninstalled click-8.3.1\n",
            "Successfully installed click-8.2.1 ray-2.52.0 rl_games-1.5.2 setproctitle-1.3.7 tensorboardX-2.6.4\n"
          ]
        }
      ],
      "source": [
        "# 1. Setup Environment\n",
        "!git clone https://github.com/dhruv0000/neural-robot-dynamics.git\n",
        "%cd neural-robot-dynamics\n",
        "!pip install -r requirements.txt\n",
        "!pip install warp-lang==1.8.0\n",
        "!pip install rl_games"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "66700684",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66700684",
        "outputId": "27e8d3a4-d11d-4747-ce84-691a55bf3b74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/neural-robot-dynamics/generate\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 64 environments: 100% 64/64 [00:00<00:00, 188.84it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 21206.10 ms  (compiled)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 575.82 ms  (compiled)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 749.49 ms  (compiled)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "  0% 0/10000 [00:00<?, ?it/s]Module utils.warp_utils 294c46a load on device 'cuda:0' took 668.76 ms  (compiled)\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 16033.30 ms  (compiled)\n",
            "12800it [00:17, 874.56it/s]              \n",
            "\n",
            "Total number of transitions generated: 12800\n",
            "12800it [00:17, 724.20it/s]\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 64 environments: 100% 64/64 [00:00<00:00, 193.59it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.81 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.48 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "  0% 0/2000 [00:00<?, ?it/s]Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.47 ms  (cached)\n",
            "Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.41 ms  (cached)\n",
            "6400it [00:00, 14674.23it/s]\n",
            "\n",
            "Total number of transitions generated: 6400\n",
            "6400it [00:00, 14638.85it/s]\n",
            "/content/neural-robot-dynamics\n"
          ]
        }
      ],
      "source": [
        "# 2. Generate Dataset\n",
        "# We generate a smaller dataset for demonstration purposes.\n",
        "\n",
        "%cd generate\n",
        "\n",
        "# Generate Training Data\n",
        "!python generate_dataset_contact_free.py --env-name Cartpole --num-transitions 10000 --dataset-dir ../data/datasets/ --dataset-name trajectory_len-100_train.hdf5 --trajectory-length 100 --num-envs 64 --seed 0\n",
        "\n",
        "# Generate Validation Data\n",
        "!python generate_dataset_contact_free.py --env-name Cartpole --num-transitions 2000 --dataset-dir ../data/datasets/ --dataset-name trajectory_len-100_valid.hdf5 --trajectory-length 100 --num-envs 64 --seed 10\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d0907b67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0907b67",
        "outputId": "fd83dcb4-d3eb-48cd-c0ad-29fb5bb36200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/neural-robot-dynamics/train\n",
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 17:57:10.041161: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763920630.060345    1294 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763920630.066117    1294 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763920630.080930    1294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920630.080952    1294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920630.080958    1294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920630.080962    1294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 17:57:10.085491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 321.42it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.05 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.44 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "number of parameters: 2.69M\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (transformer_model): GPT(\n",
            "    (transformer): ModuleDict(\n",
            "      (wte): Linear(in_features=6, out_features=192, bias=True)\n",
            "      (wpe): Embedding(32, 192)\n",
            "      (drop): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-5): 6 x Block(\n",
            "          (ln_1): LayerNorm()\n",
            "          (attn): CausalSelfAttention(\n",
            "            (c_attn): Linear(in_features=192, out_features=576, bias=False)\n",
            "            (c_proj): Linear(in_features=192, out_features=192, bias=False)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm()\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Linear(in_features=192, out_features=768, bias=False)\n",
            "            (gelu): GELU(approximate='none')\n",
            "            (c_proj): Linear(in_features=768, out_features=192, bias=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=192, out_features=192, bias=False)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  2713668\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:01<00:00,  8.46it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.51 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 1001.18 ms  (compiled)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.37 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 3698.64 ms  (compiled)\n",
            "Sampling state transitions: 100% 4/4 [00:04<00:00,  1.24s/it]\n",
            "100% 4/4 [00:00<00:00,  5.83it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.57209951, Rollout MSE Error (joint_q) = 0.01352453 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 0.572099506855011. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.89327049, itemized = {state_0: 0.00767032, state_1: 0.87654409, state_2: 0.24498109, state_3: 0.27191060, state_MSE: 0.35027654, q_error_norm: 0.28657785, qd_error_norm: 0.52501436} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 7.027 sec, time(other): 0.001 sec, time(dataloader): 0.635 sec, time(compute_loss): 0.663 sec, time(backward): 0.000 sec, time(eval): 5.711 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 0.89327049. \u001b[0m\n",
            "100% 100/100 [00:09<00:00, 10.49it/s]\n",
            "100% 11/11 [00:00<00:00, 12.12it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.26it/s]\n",
            "100% 4/4 [00:00<00:00,  6.96it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.23402007, Rollout MSE Error (joint_q) = 0.00663242 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 1 with MSE error 0.23402006924152374. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.33126404, itemized = {state_0: 0.00083377, state_1: 0.23053471, state_2: 0.10704877, state_3: 0.21781916, state_MSE: 0.13905916, q_error_norm: 0.07858131, qd_error_norm: 0.37626648} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.14883819, itemized = {state_0: 0.00019616, state_1: 0.22869390, state_2: 0.04450290, state_3: 0.10678220, state_MSE: 0.09504379, q_error_norm: 0.06703554, qd_error_norm: 0.27306598} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.406 sec, time(other): 0.024 sec, time(dataloader): 5.235 sec, time(compute_loss): 2.176 sec, time(backward): 2.960 sec, time(eval): 0.768 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.018429, device='cuda:0'), grad_norm_after_clip: tensor(0.868848, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 0.14883819. \u001b[0m\n",
            "100% 100/100 [00:09<00:00, 10.14it/s]\n",
            "100% 11/11 [00:00<00:00, 12.69it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.41it/s]\n",
            "100% 4/4 [00:00<00:00,  6.99it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.07920126, Rollout MSE Error (joint_q) = 0.00158553 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 2 with MSE error 0.07920125871896744. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.08601153, itemized = {state_0: 0.00013337, state_1: 0.09280305, state_2: 0.03220012, state_3: 0.05785245, state_MSE: 0.04574726, q_error_norm: 0.03276653, qd_error_norm: 0.20147537} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.04793230, itemized = {state_0: 0.00007494, state_1: 0.06916192, state_2: 0.01865852, state_3: 0.03166708, state_MSE: 0.02989062, q_error_norm: 0.02387684, qd_error_norm: 0.14859157} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.690 sec, time(other): 0.017 sec, time(dataloader): 5.605 sec, time(compute_loss): 2.284 sec, time(backward): 2.784 sec, time(eval): 0.758 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.001073, device='cuda:0'), grad_norm_after_clip: tensor(0.918163, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 2 with loss 0.04793230. \u001b[0m\n",
            "100% 100/100 [00:10<00:00,  9.87it/s]\n",
            "100% 11/11 [00:00<00:00, 13.99it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.81it/s]\n",
            "100% 4/4 [00:00<00:00,  6.97it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.02733320, Rollout MSE Error (joint_q) = 0.00781744 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 3 with MSE error 0.027333198115229607. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.03553375, itemized = {state_0: 0.00005818, state_1: 0.06277299, state_2: 0.01556550, state_3: 0.02121960, state_MSE: 0.02490407, q_error_norm: 0.02191646, qd_error_norm: 0.12800590} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.03533276, itemized = {state_0: 0.00013613, state_1: 0.05467754, state_2: 0.01084322, state_3: 0.02340182, state_MSE: 0.02226468, q_error_norm: 0.02453578, qd_error_norm: 0.12020665} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.909 sec, time(other): 0.027 sec, time(dataloader): 5.824 sec, time(compute_loss): 2.241 sec, time(backward): 2.793 sec, time(eval): 0.763 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(1.003369, device='cuda:0'), grad_norm_after_clip: tensor(0.844422, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 3 with loss 0.03533276. \u001b[0m\n",
            "100% 100/100 [00:10<00:00,  9.94it/s]\n",
            "100% 11/11 [00:00<00:00, 13.37it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.06it/s]\n",
            "100% 4/4 [00:00<00:00,  6.91it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.02023264, Rollout MSE Error (joint_q) = 0.00103619 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 4 with MSE error 0.02023264206945896. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.01786481, itemized = {state_0: 0.00003381, state_1: 0.04683751, state_2: 0.00745018, state_3: 0.01087055, state_MSE: 0.01629802, q_error_norm: 0.01609722, qd_error_norm: 0.09254391} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.01935512, itemized = {state_0: 0.00002691, state_1: 0.09435847, state_2: 0.00706209, state_3: 0.01284382, state_MSE: 0.02857282, q_error_norm: 0.02523972, qd_error_norm: 0.08912633} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 11.880 sec, time(other): 0.021 sec, time(dataloader): 5.957 sec, time(compute_loss): 2.212 sec, time(backward): 2.664 sec, time(eval): 0.767 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.893509, device='cuda:0'), grad_norm_after_clip: tensor(0.779324, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 4 with loss 0.01935512. \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 3. Train Baseline Model (Transformer)\n",
        "%cd train\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "\n",
        "# Load default config\n",
        "with open('cfg/Cartpole/transformer.yaml', 'r') as f:\n",
        "    cfg = yaml.safe_load(f)\n",
        "\n",
        "# Override dataset paths to point to the generated data\n",
        "cfg['algorithm']['dataset']['train_dataset_path'] = '../data/datasets/Cartpole/trajectory_len-100_train.hdf5'\n",
        "cfg['algorithm']['dataset']['valid_datasets']['exp_trajectory'] = '../data/datasets/Cartpole/trajectory_len-100_valid.hdf5'\n",
        "\n",
        "# Reduce training parameters for quick demonstration\n",
        "cfg['algorithm']['num_epochs'] = 5\n",
        "cfg['algorithm']['num_iters_per_epoch'] = 100\n",
        "\n",
        "# Save the modified config\n",
        "with open('colab_config.yaml', 'w') as f:\n",
        "    yaml.dump(cfg, f)\n",
        "\n",
        "# Run training\n",
        "!python train.py --cfg colab_config.yaml --logdir ../data/logs/baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c69c48d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c69c48d1",
        "outputId": "e824e2e3-c98e-4263-ed6a-2526e3af093b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 17:58:26.902536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763920706.934916    2077 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763920706.944724    2077 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763920706.968518    2077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920706.968552    2077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920706.968560    2077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920706.968567    2077 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 17:58:26.975063: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 317.67it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.96 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.41 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.46 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (mamba_model): Mamba(\n",
            "    (embedding): Linear(in_features=6, out_features=192, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x MambaBlock(\n",
            "        (in_proj): Linear(in_features=192, out_features=768, bias=False)\n",
            "        (conv1d): Conv1d(384, 384, kernel_size=(4,), stride=(1,), padding=(3,), groups=384)\n",
            "        (x_proj): Linear(in_features=384, out_features=44, bias=False)\n",
            "        (dt_proj): Linear(in_features=12, out_features=384, bias=True)\n",
            "        (out_proj): Linear(in_features=384, out_features=192, bias=False)\n",
            "      )\n",
            "    )\n",
            "    (norm_f): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  1523460\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:02<00:00,  5.14it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.89 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.51 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.51 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.73 ms  (cached)\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 14.20it/s]\n",
            "100% 4/4 [00:01<00:00,  2.51it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.43373597, Rollout MSE Error (joint_q) = 0.06781688 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 1.433735966682434. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 1.00439644, itemized = {state_0: 0.00870172, state_1: 0.91801999, state_2: 0.25972505, state_3: 0.32500295, state_MSE: 0.37786241, q_error_norm: 0.30157635, qd_error_norm: 0.58810208} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 4.068 sec, time(other): 0.001 sec, time(dataloader): 0.973 sec, time(compute_loss): 1.162 sec, time(backward): 0.000 sec, time(eval): 1.914 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 1.00439644. \u001b[0m\n",
            "100% 100/100 [00:48<00:00,  2.05it/s]\n",
            "100% 11/11 [00:01<00:00,  7.97it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 23.77it/s]\n",
            "100% 4/4 [00:01<00:00,  2.50it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.01975624, Rollout MSE Error (joint_q) = 0.00050369 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 1 with MSE error 0.019756242632865906. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.17899710, itemized = {state_0: 0.00046378, state_1: 0.13165365, state_2: 0.06653225, state_3: 0.10681804, state_MSE: 0.07636691, q_error_norm: 0.04521545, qd_error_norm: 0.23944275} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.02070835, itemized = {state_0: 0.00004104, state_1: 0.05182135, state_2: 0.00865172, state_3: 0.01206261, state_MSE: 0.01814418, q_error_norm: 0.01769918, qd_error_norm: 0.09432975} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 52.256 sec, time(other): 0.383 sec, time(dataloader): 18.669 sec, time(compute_loss): 7.505 sec, time(backward): 23.485 sec, time(eval): 1.813 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.366601, device='cuda:0'), grad_norm_after_clip: tensor(0.351471, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 0.02070835. \u001b[0m\n",
            "100% 100/100 [00:48<00:00,  2.05it/s]\n",
            "100% 11/11 [00:01<00:00,  8.37it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 26.64it/s]\n",
            "100% 4/4 [00:01<00:00,  2.48it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00756090, Rollout MSE Error (joint_q) = 0.00031875 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 2 with MSE error 0.00756090460345149. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00893516, itemized = {state_0: 0.00002393, state_1: 0.04518652, state_2: 0.00386501, state_3: 0.00466609, state_MSE: 0.01343539, q_error_norm: 0.01450096, qd_error_norm: 0.06293787} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00797624, itemized = {state_0: 0.00001570, state_1: 0.06016227, state_2: 0.00303988, state_3: 0.00494928, state_MSE: 0.01704178, q_error_norm: 0.01584079, qd_error_norm: 0.05500800} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 52.333 sec, time(other): 0.376 sec, time(dataloader): 18.807 sec, time(compute_loss): 7.603 sec, time(backward): 23.323 sec, time(eval): 1.808 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.155297, device='cuda:0'), grad_norm_after_clip: tensor(0.155297, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 2 with loss 0.00797624. \u001b[0m\n",
            "100% 100/100 [00:48<00:00,  2.04it/s]\n",
            "100% 11/11 [00:01<00:00,  8.30it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.45it/s]\n",
            "100% 4/4 [00:01<00:00,  2.39it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00233163, Rollout MSE Error (joint_q) = 0.00021908 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 3 with MSE error 0.0023316273000091314. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00328931, itemized = {state_0: 0.00001129, state_1: 0.02878337, state_2: 0.00129734, state_3: 0.00167763, state_MSE: 0.00794240, q_error_norm: 0.00989465, qd_error_norm: 0.03948760} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00542831, itemized = {state_0: 0.00001065, state_1: 0.05317619, state_2: 0.00172920, state_3: 0.00372670, state_MSE: 0.01466068, q_error_norm: 0.01371052, qd_error_norm: 0.03887138} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 52.566 sec, time(other): 0.369 sec, time(dataloader): 19.017 sec, time(compute_loss): 7.576 sec, time(backward): 23.331 sec, time(eval): 1.861 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.083606, device='cuda:0'), grad_norm_after_clip: tensor(0.083606, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 3 with loss 0.00542831. \u001b[0m\n",
            "100% 100/100 [00:49<00:00,  2.02it/s]\n",
            "100% 11/11 [00:01<00:00,  8.22it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.07it/s]\n",
            "100% 4/4 [00:01<00:00,  2.46it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.00168614, Rollout MSE Error (joint_q) = 0.00012483 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 4 with MSE error 0.0016861445037648082. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 0.00182711, itemized = {state_0: 0.00000754, state_1: 0.02463967, state_2: 0.00064641, state_3: 0.00091838, state_MSE: 0.00655300, q_error_norm: 0.00838156, qd_error_norm: 0.02976539} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 0.00459591, itemized = {state_0: 0.00000805, state_1: 0.05528026, state_2: 0.00134581, state_3: 0.00333575, state_MSE: 0.01499247, q_error_norm: 0.01346235, qd_error_norm: 0.03372930} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 52.923 sec, time(other): 0.376 sec, time(dataloader): 19.318 sec, time(compute_loss): 7.665 sec, time(backward): 23.346 sec, time(eval): 1.801 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(0.043410, device='cuda:0'), grad_norm_after_clip: tensor(0.043410, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 4 with loss 0.00459591. \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 4. Train Mamba Model\n",
        "# We use the same config but add the --novelty mamba flag\n",
        "!python train.py --cfg colab_config.yaml --novelty mamba --logdir ../data/logs/mamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a32b3a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a32b3a28",
        "outputId": "7f122ca2-52a5-44d8-d0ca-6f4c6e70653c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warp DeprecationWarning: The `warp.sim` module is deprecated and will be removed in v1.10. Please transition to using the forthcoming Newton library instead.\n",
            "Warp 1.8.0 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.8.0\n",
            "2025-11-23 18:02:18.407934: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1763920938.427338    3493 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1763920938.433641    3493 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1763920938.448743    3493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920938.448777    3493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920938.448780    3493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1763920938.448784    3493 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-11-23 18:02:18.453441: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[96m [NeuralEnvironment] Creating abstract contact environment: Cartpole. \u001b[0m\n",
            "Creating 512 environments: 100% 512/512 [00:01<00:00, 321.82it/s]\n",
            "Module warp.sim.integrator_featherstone 18b3327 load on device 'cuda:0' took 4.31 ms  (cached)\n",
            "Module envs.abstract_contact_environment 8e8d790 load on device 'cuda:0' took 0.41 ms  (cached)\n",
            "Module integrators.integrator_neural ee402cd load on device 'cuda:0' took 0.50 ms  (cached)\n",
            "\u001b[91m [NeuralEnvironment] Created a DUMMY Neural Integrator. \u001b[0m\n",
            "number of parameters: 2.69M\n",
            "Model = \n",
            " ModelMixedInput(\n",
            "  (encoders): ModuleDict(\n",
            "    (low_dim): MLPBase(\n",
            "      (body): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (transformer_model): GPT(\n",
            "    (transformer): ModuleDict(\n",
            "      (wte): Linear(in_features=6, out_features=192, bias=True)\n",
            "      (wpe): Embedding(32, 192)\n",
            "      (drop): Dropout(p=0.0, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-5): 6 x Block(\n",
            "          (ln_1): LayerNorm()\n",
            "          (attn): CausalSelfAttention(\n",
            "            (c_attn): Linear(in_features=192, out_features=576, bias=False)\n",
            "            (c_proj): Linear(in_features=192, out_features=192, bias=False)\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm()\n",
            "          (mlp): MLP(\n",
            "            (c_fc): Linear(in_features=192, out_features=768, bias=False)\n",
            "            (gelu): GELU(approximate='none')\n",
            "            (c_proj): Linear(in_features=768, out_features=192, bias=False)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm()\n",
            "    )\n",
            "    (lm_head): Linear(in_features=192, out_features=192, bias=False)\n",
            "  )\n",
            "  (model): MLPDeterministic(\n",
            "    (feature_net): MLPBase(\n",
            "      (body): Sequential(\n",
            "        (0): Linear(in_features=192, out_features=64, bias=True)\n",
            "        (1): ReLU()\n",
            "      )\n",
            "    )\n",
            "    (output_net): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "# Model Parameters =  2713668\n",
            "Computing dataset statistics...\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Finished computing dataset statistics...\n",
            "100% 11/11 [00:01<00:00,  5.56it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions:   0% 0/4 [00:00<?, ?it/s]Module warp.sim.articulation 770a52a load on device 'cuda:0' took 1.98 ms  (cached)\n",
            "Module envs.warp_sim_envs.env_cartpole 01fd57b load on device 'cuda:0' took 0.50 ms  (cached)\n",
            "Module utils.warp_utils 294c46a load on device 'cuda:0' took 0.52 ms  (cached)\n",
            "Module envs.warp_sim_envs.utils d93eb17 load on device 'cuda:0' took 0.94 ms  (cached)\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 13.00it/s]\n",
            "100% 4/4 [00:00<00:00,  6.09it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 0.57209951, Rollout MSE Error (joint_q) = 0.01352453 \u001b[0m\n",
            "\u001b[92m Save Best Eval Model at Epoch 0 with MSE error 0.572099506855011. \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 0 \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 3.68339885, itemized = {state_MSE: 3.68339885} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 3.024 sec, time(other): 0.000 sec, time(dataloader): 0.601 sec, time(compute_loss): 1.375 sec, time(backward): 0.000 sec, time(eval): 1.031 sec \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 0 with loss 3.68339885. \u001b[0m\n",
            "100% 100/100 [00:33<00:00,  2.96it/s]\n",
            "100% 11/11 [00:01<00:00,  6.54it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 20.30it/s]\n",
            "100% 4/4 [00:00<00:00,  5.89it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.21425354, Rollout MSE Error (joint_q) = 0.33249471 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 1 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.95338409, itemized = {state_MSE: 2.95338409} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.29789838, itemized = {state_MSE: 2.29789838} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 36.823 sec, time(other): 0.006 sec, time(dataloader): 4.403 sec, time(compute_loss): 10.196 sec, time(backward): 20.859 sec, time(eval): 0.893 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(9.414604, device='cuda:0'), grad_norm_after_clip: tensor(1.000000, device='cuda:0')} \u001b[0m\n",
            "\u001b[92m Save Best Valid exp_trajectory Model at Epoch 1 with loss 2.29789838. \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.86it/s]\n",
            "100% 11/11 [00:01<00:00,  5.77it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 28.73it/s]\n",
            "100% 4/4 [00:00<00:00,  6.57it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 2.11347532, Rollout MSE Error (joint_q) = 1.35214865 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 2 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.64865326, itemized = {state_MSE: 2.64865326} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.62989443, itemized = {state_MSE: 2.62989443} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 38.139 sec, time(other): 0.006 sec, time(dataloader): 5.379 sec, time(compute_loss): 10.715 sec, time(backward): 20.767 sec, time(eval): 0.765 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(33.423061, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.87it/s]\n",
            "100% 11/11 [00:01<00:00,  7.02it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 26.77it/s]\n",
            "100% 4/4 [00:00<00:00,  6.65it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 1.32412612, Rollout MSE Error (joint_q) = 0.42347786 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 3 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.85362000, itemized = {state_MSE: 2.85362000} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.58044659, itemized = {state_MSE: 2.58044659} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 37.716 sec, time(other): 0.006 sec, time(dataloader): 5.207 sec, time(compute_loss): 10.358 sec, time(backward): 20.868 sec, time(eval): 0.766 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(220.961975, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n",
            "100% 100/100 [00:34<00:00,  2.87it/s]\n",
            "100% 11/11 [00:01<00:00,  7.00it/s]\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "Evaluating\n",
            "Sampling state transitions: 100% 4/4 [00:00<00:00, 27.27it/s]\n",
            "100% 4/4 [00:00<00:00,  6.59it/s]\n",
            "\u001b[37m [Evaluate], Num Rollouts = 2048, Rollout Length = 10, Rollout MSE Error = 2.38235402, Rollout MSE Error (joint_q) = 1.49744999 \u001b[0m\n",
            "\u001b[96m ---------------------------------------------------------------------------------------------------- \u001b[0m\n",
            "\u001b[96m Epoch 4 \u001b[0m\n",
            "\u001b[96m [Train] loss = 2.92595856, itemized = {state_MSE: 2.92595856} \u001b[0m\n",
            "\u001b[96m [Valid] dataset [exp_trajectory]: loss = 2.81243844, itemized = {state_MSE: 2.81243844} \u001b[0m\n",
            "\u001b[96m [Time Report] time(epoch): 37.572 sec, time(other): 0.006 sec, time(dataloader): 4.917 sec, time(compute_loss): 10.546 sec, time(backward): 20.935 sec, time(eval): 0.769 sec \u001b[0m\n",
            "\u001b[96m [Grad Info] {grad_norm_before_clip: tensor(529.858093, device='cuda:0'), grad_norm_after_clip: tensor(1., device='cuda:0')} \u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# 5. Train Unroll Model\n",
        "# We use the same config but add the --novelty unroll flag\n",
        "!python train.py --cfg colab_config.yaml --novelty unroll --logdir ../data/logs/unroll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "638f64f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638f64f6",
        "outputId": "e1ae1e66-e7bd-4791-cbe7-2fa86014ebc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating Baseline Model...\n",
            "Using model: ../data/logs/baseline/11-23-2025-17-57-17/nn/best_eval_model.pt\n",
            "Evaluating Mamba Model...\n",
            "Using model: ../data/logs/mamba/11-23-2025-17-58-33/nn/best_eval_model.pt\n",
            "Evaluating Unroll Model...\n",
            "Using model: ../data/logs/unroll/11-23-2025-18-02-25/nn/best_eval_model.pt\n"
          ]
        }
      ],
      "source": [
        "# 6. RL Evaluation\n",
        "# Evaluate the trained models using the pretrained RL policy\n",
        "import os\n",
        "import glob\n",
        "import subprocess\n",
        "\n",
        "def find_latest_model(model_type):\n",
        "    base_log_dir = f'../data/logs/{model_type}'\n",
        "    if not os.path.exists(base_log_dir):\n",
        "        print(f'Log dir not found: {base_log_dir}')\n",
        "        return None\n",
        "    dirs = [d for d in glob.glob(os.path.join(base_log_dir, '*')) if os.path.isdir(d)]\n",
        "    if not dirs:\n",
        "        print(f'No logs found for {model_type}')\n",
        "        return None\n",
        "    latest_dir = sorted(dirs)[-1]\n",
        "    model_path = os.path.join(latest_dir, 'nn', 'best_eval_model.pt')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f'Model file not found: {model_path}')\n",
        "        return None\n",
        "    return model_path\n",
        "\n",
        "models = ['baseline', 'mamba', 'unroll']\n",
        "for model in models:\n",
        "    print(f'Evaluating {model.capitalize()} Model...')\n",
        "    model_path = find_latest_model(model)\n",
        "    if model_path:\n",
        "        print(f'Using model: {model_path}')\n",
        "\n",
        "        # Convert paths to absolute to avoid issues with subprocess cwd\n",
        "        abs_model_path = os.path.abspath(model_path)\n",
        "        abs_playback_path = os.path.abspath('../pretrained_models/RL_policies/Cartpole/0/nn/CartpolePPO.pth')\n",
        "        abs_rl_cfg_path = os.path.abspath('../eval/eval_rl/cfg/Cartpole/cartpole.yaml')\n",
        "\n",
        "        cmd = [\n",
        "            'python', 'run_rl.py',\n",
        "            '--rl-cfg', abs_rl_cfg_path,\n",
        "            '--playback', abs_playback_path,\n",
        "            '--num-envs', '1',\n",
        "            '--num-games', '5',\n",
        "            '--env-mode', 'neural',\n",
        "            '--nerd-model-path', abs_model_path\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            subprocess.run(cmd, cwd='../eval/eval_rl', check=True, capture_output=True, text=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f'Error running RL evaluation for {model}:')\n",
        "            print('STDOUT:', e.stdout)\n",
        "            print('STDERR:', e.stderr)\n",
        "            raise e\n",
        "    else:\n",
        "        print(f'Skipping {model} evaluation.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
